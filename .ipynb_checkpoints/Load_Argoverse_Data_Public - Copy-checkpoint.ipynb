{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\"\"\"Change to the data folder\"\"\"\n",
    "new_path = \"../new_train/\"\n",
    "val_path = \"../new_val_in/\"\n",
    "\n",
    "# number of sequences in each dataset\n",
    "# train:205942  val:3200 test: 36272 \n",
    "# sequences sampled at 10HZ rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, data_path: str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pkl_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pkl_path = self.pkl_list[idx]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "# intialize a dataset\n",
    "train_dataset  = ArgoverseDataset(data_path=new_path)\n",
    "val_dataset = ArgoverseDataset(data_path=val_path)\n",
    "#print((val_dataset[0]))\n",
    "#print(len(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a loader to enable batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 1\n",
    "\n",
    "def my_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    inp = [numpy.dstack([scene['p_in'], scene['v_in']]) for scene in batch]\n",
    "    out = [numpy.dstack([scene['p_out'], scene['v_out']]) for scene in batch]\n",
    "#     inp = [scene['p_in'] for scene in batch]\n",
    "#     out = [scene['p_out'] for scene in batch]\n",
    "#     print(inp.size)\n",
    "#     print(\"gap\")\n",
    "#     print(out.size)\n",
    "    #inp = np.concatenate((inp, out), axis=0)\n",
    "    inp = torch.LongTensor(inp)\n",
    "#     print(inp.shape)\n",
    "#     print(\"after\")\n",
    "#     print(inp)\n",
    "    out = torch.LongTensor(out)\n",
    "    return [inp, out]\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def val_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    inp = [numpy.dstack([scene['p_in'], scene['v_in']]) for scene in batch]\n",
    "    inp = torch.LongTensor(inp)\n",
    "    return inp\n",
    "\n",
    "val_loader = DataLoader(val_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 120\n",
    "hidden_size = 240\n",
    "hidden_size2 = 10\n",
    "out_len = 30\n",
    "attn_heads = 3\n",
    "class EncDecModel(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(EncDecModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "#         self.multihead_attn = nn.MultiheadAttention(hidden_size, attn_heads)\n",
    "        self.attn = nn.Linear(hidden_size, 19)\n",
    "        self.gru_cell = nn.GRUCell(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, input_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        output, last_hidden = self.gru(x)\n",
    "        last = last_hidden\n",
    "#         print(\"Shape of last before loop: \", last.shape)\n",
    "        p_out = []\n",
    "        for i in range(out_len):\n",
    "            last = last.reshape(1, 16, hidden_size)\n",
    "            attn_weights = F.softmax(self.attn(last))\n",
    "#             print(\"For BMM: \", attn_weights.shape, output.shape)\n",
    "            attn_applied = torch.bmm(attn_weights.transpose(0,1), output.transpose(0,1))\n",
    "#             print(\"For BMM2: \", attn_weights.shape, output.shape)\n",
    "\n",
    "#             print(\"For final gru: \", attn_applied.transpose(1,2).reshape(16,20).shape, last.transpose(0,1).transpose(1,2).reshape(16,20).shape)\n",
    "            last = self.gru_cell(attn_applied.transpose(1,2).reshape(16,hidden_size), last.transpose(0,1).transpose(1,2).reshape(16,hidden_size))\n",
    "#             print(\"Shape of last: \", last.shape)\n",
    "            p_out.append(self.out(last))\n",
    "        \n",
    "        return torch.stack(p_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "torch.Size([19, 16, 15])\n",
      "torch.Size([19, 16, 15])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 120, got 15",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-d93b0a8a65c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m#         inp[0][i] = torch.cat((inp[0][i], out[0][i]), 0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;31m#     print(inp[0].shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m#print(output)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda31\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-082dec54f094>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mlast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlast_hidden\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#         print(\"Shape of last before loop: \", last.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda31\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda31\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    735\u001b[0m             \u001b[0mhx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    738\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[1;32m~\\anaconda31\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda31\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    176\u001b[0m                     expected_input_dim, input.dim()))\n\u001b[0;32m    177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m             raise RuntimeError(\n\u001b[0m\u001b[0;32m    179\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[0;32m    180\u001b[0m                     self.input_size, input.size(-1)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 120, got 15"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "agent_id = 0\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "# device = \"cuda:0\"\n",
    "device = \"cpu\"\n",
    "input_dim = 4    # input dimension\n",
    "hidden_dim = 100  # hidden layer dimension\n",
    "layer_dim = 1     # number of hidden layers\n",
    "output_dim = 4   # output dimension\n",
    "\n",
    "n_epochs = 100\n",
    "lr=0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "#model = RNNModel(input_dim, output_dim, hidden_dim, layer_dim).to(device)\n",
    "model = EncDecModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,momentum=momentum)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "model.train()\n",
    "print(\"test\")\n",
    "for i_batch, sample_batch in enumerate(train_loader):\n",
    "    inp, out = sample_batch\n",
    "    #make_a_histogram(sample_batch, agent_id, xPos, yPos, xVel, yVel)\n",
    "    \"\"\"TODO:\n",
    "      Deep learning model\n",
    "      training routine\n",
    "    \"\"\"\n",
    "    inp = inp.transpose(0,2)\n",
    "    inp = inp.reshape(19, 16, -1)\n",
    "    inp, out = inp.to(device), out.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    print(inp.shape)\n",
    "#     for i in range (60):\n",
    "#         inp[0][i] = torch.cat((inp[0][i], out[0][i]), 0)\n",
    "#     print(inp[0].shape)\n",
    "    output = model(inp.float())\n",
    "    print(out.shape)\n",
    "    #print(output)\n",
    "#     output = output.reshape(60,30,4)\n",
    "    print(output.shape)\n",
    "    loss = nn.MSELoss()\n",
    "    output = output.reshape(30, 16, 60, 2)\n",
    "    output = output.transpose(0,2).transpose(0,1)\n",
    "#     print(output.shape, out.shape)\n",
    "\n",
    "    loss_val = loss(output, out.float())\n",
    "    print(loss_val)\n",
    "    loss_val.backward()\n",
    "    optimizer.step()\n",
    "    break\n",
    "#     show_sample_batch(sample_batch, agent_id)\n",
    "#     break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Number of hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # RNN\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True, nonlinearity='relu')\n",
    "        #print(self.rnn)\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        #print(hidden.shape)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "        \"\"\"\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "            \n",
    "        # One time step\n",
    "        print(x)\n",
    "        #print(\"gap\")\n",
    "        #print(h0.shape)\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n",
    "        \"\"\"\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_a_histogram(sample_batch, agent_id, xPos, yPos, xVel, yVel):\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    #agent_sz = inp.size(1)\n",
    "    \n",
    "    for i in range(batch_sz):\n",
    "        #hist_data_xPos = np.zeros((60,19));\n",
    "        #hist_data_yPos = np.zeros((60,19));\n",
    "        #hist_data_xVel = np.zeros((60,19));\n",
    "        hist_data_yVel = np.zeros((60,19));\n",
    "        \n",
    "        for j in range(60):\n",
    "            #hist_data_xPos[j] = (inp[i, j,:,0])\n",
    "            #hist_data_yPos[j] = (inp[i, j,:,1])\n",
    "            #hist_data_xVel[j] = (inp[i, j,:,2])\n",
    "            hist_data_yVel[j] = (inp[i, j,:,3])\n",
    "            \n",
    "        for j in range(len(hist_data_yVel)):\n",
    "            for k in range(len(hist_data_yVel[j])):\n",
    "                #xPos.append(hist_data_xPos[j][k])\n",
    "                #yPos.append(hist_data_yPos[j][k])\n",
    "                #xVel.append(hist_data_xVel[j][k])\n",
    "                yVel.append(hist_data_yVel[j][k])\n",
    "    \n",
    "    \"\"\"\n",
    "    hist_data_xPos = np.zeros((60,19));\n",
    "    hist_data_yPos = np.zeros((60,19));\n",
    "    hist_data_xVel = np.zeros((60,19));\n",
    "    hist_data_yVel = np.zeros((60,19));\n",
    "    \n",
    "    for i in range(60):\n",
    "        hist_data_xPos[i] = (inp[0, i,:,0])\n",
    "        hist_data_yPos[i] = (inp[0, i,:,1])\n",
    "        hist_data_xVel[i] = (inp[0, i,:,2])\n",
    "        hist_data_yVel[i] = (inp[0, i,:,3])\n",
    "    \n",
    "    xPos = np.zeros(60*19)\n",
    "    for i in range(len(hist_data_xPos)):\n",
    "        for j in range(len(hist_data_xPos[i])):\n",
    "            xPos[i*19+j] = hist_data_xPos[i][j]\n",
    "    \n",
    "    #hist_data_xPos = hist_data_xPos.flatten()\n",
    "    hist_data_yVel = hist_data_yPos.flatten()\n",
    "    hist_data_xPos = hist_data_xVel.flatten()\n",
    "    hist_data_yVel = hist_data_yVel.flatten()\n",
    "    #print(xPos)\n",
    "    \n",
    "    n,bins,patches = plt.hist(x=xPos,bins='auto',alpha=0.7,rwidth=0.85)\n",
    "    plt.grid(axis='y',alpha=0.75)\n",
    "    maxfreq = n.max()\n",
    "    plt.ylim(ymax=np.ceil(maxfreq/10) * 10 if maxfreq % 10 else maxfreq + 10)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_batch(sample_batch, agent_id):\n",
    "    \"\"\"visualize the trajectory for a batch of samples with a randon agent\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i, agent_id,:,0], inp[i, agent_id,:,1])\n",
    "        axs[i].scatter(out[i, agent_id,:,0], out[i, agent_id,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the batch of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "test\n",
      "torch.Size([60, 19, 4])\n",
      "torch.Size([1, 60, 12])\n",
      "torch.Size([1, 60, 19, 4])\n",
      "out tensor([[ 3.2772e+03,  1.9778e+03, -1.2366e-01,  3.3896e-01],\n",
      "        [ 3.2772e+03,  1.9777e+03,  8.5506e-02, -6.9142e-01],\n",
      "        [ 3.2771e+03,  1.9778e+03, -1.6988e-01,  6.7708e-01],\n",
      "        [ 3.2772e+03,  1.9778e+03,  6.6099e-01,  7.6622e-01],\n",
      "        [ 3.2771e+03,  1.9777e+03, -9.7746e-01, -1.1108e+00],\n",
      "        [ 3.2771e+03,  1.9778e+03, -5.1770e-03,  3.5186e-01],\n",
      "        [ 3.2771e+03,  1.9777e+03, -1.2338e-01, -4.8915e-01],\n",
      "        [ 3.2771e+03,  1.9777e+03,  2.3385e-02,  5.6519e-02],\n",
      "        [ 3.2772e+03,  1.9778e+03,  7.8288e-01,  4.5554e-01],\n",
      "        [ 3.2772e+03,  1.9778e+03, -2.3511e-01,  3.2175e-02],\n",
      "        [ 3.2771e+03,  1.9777e+03, -3.8379e-01, -6.6550e-01],\n",
      "        [ 3.2771e+03,  1.9777e+03, -1.0223e-01, -4.6996e-02],\n",
      "        [ 3.2772e+03,  1.9777e+03,  7.5663e-01,  1.8636e-01],\n",
      "        [ 3.2771e+03,  1.9777e+03, -5.5025e-01,  5.1680e-02],\n",
      "        [ 3.2771e+03,  1.9777e+03, -1.4912e-01, -1.9836e-02],\n",
      "        [ 3.2771e+03,  1.9777e+03, -2.4805e-01, -3.4992e-01],\n",
      "        [ 3.2772e+03,  1.9778e+03,  9.9766e-01,  6.8928e-01],\n",
      "        [ 3.2772e+03,  1.9778e+03, -5.7259e-02,  4.5092e-01],\n",
      "        [ 3.2772e+03,  1.9778e+03, -1.1269e-01, -6.9720e-02],\n",
      "        [ 3.2771e+03,  1.9776e+03, -1.0526e+00, -2.0687e+00],\n",
      "        [ 3.2772e+03,  1.9777e+03,  7.9787e-01,  1.4458e+00],\n",
      "        [ 3.2771e+03,  1.9777e+03, -2.6180e-02, -1.1960e-01],\n",
      "        [ 3.2771e+03,  1.9777e+03, -4.8560e-01, -2.3010e-01],\n",
      "        [ 3.2772e+03,  1.9777e+03,  6.4186e-01,  5.5418e-01],\n",
      "        [ 3.2771e+03,  1.9777e+03, -4.8919e-01, -4.3476e-01],\n",
      "        [ 3.2772e+03,  1.9778e+03,  6.6857e-01,  8.2498e-01],\n",
      "        [ 3.2772e+03,  1.9777e+03,  1.5951e-01, -4.5063e-01],\n",
      "        [ 3.2771e+03,  1.9777e+03, -6.4413e-01,  2.2651e-02],\n",
      "        [ 3.2771e+03,  1.9777e+03,  1.5696e-01, -5.6332e-02],\n",
      "        [ 3.2771e+03,  1.9777e+03,  0.0000e+00,  0.0000e+00]])\n",
      "outlist torch.Size([7200])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-fcb0f3c08bda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m60\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m30\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda31\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2262\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[0;32m   2263\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2264\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2265\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2266\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "agent_id = 0\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "device = \"cpu\"\n",
    "input_dim = 4    # input dimension\n",
    "hidden_dim = 100  # hidden layer dimension\n",
    "layer_dim = 1     # number of hidden layers\n",
    "output_dim = 4   # output dimension\n",
    "\n",
    "n_epochs = 100\n",
    "lr=0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "#model = RNNModel(input_dim, output_dim, hidden_dim, layer_dim).to(device)\n",
    "model = RNNModel(input_size=input_dim, output_size=output_dim, hidden_dim=12, n_layers=1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,momentum=momentum)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "model.train()\n",
    "print(\"test\")\n",
    "for i_batch, sample_batch in enumerate(train_loader):\n",
    "    print(\"test\")\n",
    "    inp, out = sample_batch\n",
    "    #make_a_histogram(sample_batch, agent_id, xPos, yPos, xVel, yVel)\n",
    "    \"\"\"TODO:\n",
    "      Deep learning model\n",
    "      training routine\n",
    "    \"\"\"\n",
    "    inp, out = inp.to(device), out.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    print(inp[0].shape)\n",
    "    outList = np.zeros((1,60,30,4))\n",
    "#     print(outList[0])\n",
    "#     print(inp.shape)\n",
    "    for i in range(30):\n",
    "        output,hidden = model(inp[0])\n",
    "        output = output.reshape(1,60,19,4)\n",
    "#         print(\"output\",output.shape)\n",
    "        for j in range(60):\n",
    "#             print(output[0][j][18])\n",
    "#             print(outList[0][j][18])\n",
    "            outList[0][j][i] = output[0][j][18].detach().numpy()\n",
    "        for k in range(60):\n",
    "            for j in range(1,19):\n",
    "                inp[0][k][j-1] = inp[0][k][j]\n",
    "            inp[0][k][18] = output[0][k][18]\n",
    "    print(hidden.shape)\n",
    "    print(output.shape)\n",
    "    outList = outList.reshape(60,30,4)\n",
    "    print(\"out\",out[0][1])\n",
    "    outList = Variable(torch.from_numpy(outList))\n",
    "    print(\"outlist\",out.view(-1).shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    loss = F.nll_loss(out.view(60 * 30 * 4,1) , out.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    show_sample_batch(sample_batch, agent_id)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "  # input_size: Dimensionality of input feature vector.\n",
    "  # num_classes: The number of classes in the classification problem.\n",
    "  def __init__(self, input_size, num_classes):\n",
    "    # Always call the superclass (nn.Module) constructor first!\n",
    "    super(LogisticRegression, self).__init__()\n",
    "    # Set up the linear transform\n",
    "    self.linear = nn.Linear(input_size, num_classes)\n",
    "\n",
    "  # Forward's sole argument is the input.\n",
    "  # input is of shape (batch_size, input_size)\n",
    "  def forward(self, x):\n",
    "    # Apply the linear transform.\n",
    "    # out is of shape (batch_size, num_classes)\n",
    "    out = self.linear(x)\n",
    "    # Softmax the out tensor to get a log-probability distribution\n",
    "    # over classes for each example.\n",
    "    return out\n",
    "\n",
    "\n",
    "# Binary classifiation\n",
    "num_outputs = 2\n",
    "num_input_features = 2\n",
    "\n",
    "# Create the logistic regression model\n",
    "logreg_clf = LogisticRegression(num_input_features, num_outputs)\n",
    "\n",
    "print(logreg_clf)\n",
    "\n",
    "\n",
    "lr_rate = 0.001\n",
    "\n",
    "X = torch.Tensor([[0,0],[0,1], [1,0], [1,1]])\n",
    "Y = torch.Tensor([0,1,1,0]).view(-1,1) #view is similar to numpy.reshape()\n",
    "\n",
    "# Run the forward pass of the logistic regression model\n",
    "sample_output = logreg_clf(X) #completely random at the moment\n",
    "print(X)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss() # computes softmax and then the cross entropy\n",
    "optimizer = torch.optim.SGD(logreg_clf.parameters(), lr=lr_rate)\n",
    "\n",
    "\n",
    "\n",
    "from torch.autograd import Variable\n",
    "#training loop:\n",
    "\n",
    "epochs = 201 #how many times we go through the training set\n",
    "steps = X.size(0) #steps = 4; we have 4 training examples\n",
    "\n",
    "for i in range(epochs):\n",
    "    for j in range(steps):\n",
    "        #sample from the training set:\n",
    "        data_point = np.random.randint(X.size(0))\n",
    "        x_var = Variable(X[data_point], requires_grad=False).unsqueeze(0)\n",
    "        y_var = Variable(Y[data_point]).long()\n",
    "\n",
    "        optimizer.zero_grad() # zero the gradient buffers\n",
    "        y_hat = logreg_clf(x_var) #get the output from the model\n",
    "        print(\"yhat\",y_hat)\n",
    "        print(\"yvar\",y_var)\n",
    "        loss = loss_function(y_hat, y_var) #calculate the loss\n",
    "        loss.backward() #backprop\n",
    "        optimizer.step() #does the update\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        print (\"Epoch: {0}, Loss: {1}, \".format(i, loss.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
