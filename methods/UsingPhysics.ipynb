{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\"\"\"Change to the data folder\"\"\"\n",
    "new_path = \"../new_train/\"\n",
    "val_path = \"../new_val_in/\"\n",
    "\n",
    "# number of sequences in each dataset\n",
    "# train:205942  val:3200 test: 36272 \n",
    "# sequences sampled at 10HZ rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, data_path: str, val=False, transform=True,scalers=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "#         if transform:\n",
    "#             self.hasFit = False\n",
    "#             self.scaler = [MinMaxScaler() for i in range(6)]\n",
    "#             self.columns = ['p_in','v_in','p_out','v_out','lane','lane_norm']\n",
    "#         if scalers:\n",
    "#             self.scaler = scalers\n",
    "#             self.hasFit = True\n",
    "        self.val = val\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pkl_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pkl_path = self.pkl_list[idx]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "train_dataset  = ArgoverseDataset(data_path=new_path)\n",
    "val_dataset = ArgoverseDataset(data_path=val_path,val=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a loader to enable batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 1\n",
    "\n",
    "def closest_node(node, nodes):\n",
    "    deltas = nodes - node\n",
    "    dist_2 = np.einsum('ij,ij->i', deltas, deltas)\n",
    "    return np.argmin(dist_2)\n",
    "\n",
    "def my_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    inpTotal = []\n",
    "    out = []\n",
    "    city = []\n",
    "#     print(\"pIn\",batch[0]['p_in'])\n",
    "    numbRows = 60\n",
    "    for scene in batch:\n",
    "        \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "\n",
    "        agentIndex = numpy.where(scene['track_id'] == scene['agent_id'])[0][0]\n",
    "        \n",
    "    \n",
    "        return [scene['p_in'][agentIndex],scene['v_in'][agentIndex],agentIndex,scene['p_out'][agentIndex],scene['v_out'][agentIndex]]\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 1\n",
    "def val_collate(batch):\n",
    "    agentIds = []\n",
    "    trackIds = []\n",
    "    sceneIdxs = []\n",
    "    inpTotal = []\n",
    "    city = []\n",
    "    numbRows = 60\n",
    "    for scene in batch:\n",
    "        \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "        agentIds.append(scene['agent_id'])\n",
    "        trackIds.append(scene['track_id'])\n",
    "        sceneIdxs.append(scene['scene_idx'])\n",
    "\n",
    "        agentIndex = numpy.where(scene['track_id'] == scene['agent_id'])[0][0]\n",
    "       \n",
    "\n",
    "        return [scene['p_in'][agentIndex],scene['v_in'][agentIndex],sceneIdxs,agentIds,trackIds]\n",
    "\n",
    "val_loader = DataLoader(val_dataset,batch_size=batch_sz, shuffle = False, collate_fn=val_collate, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "increment  0.25  loss  1.5576459169387817\n",
      "increment  0.251  loss  1.5592690706253052\n",
      "increment  0.252  loss  1.560890793800354\n",
      "increment  0.253  loss  1.5625195503234863\n",
      "increment  0.254  loss  1.564119815826416\n",
      "increment  0.255  loss  1.565721035003662\n",
      "increment  0.256  loss  1.567312240600586\n",
      "increment  0.257  loss  1.5688890218734741\n",
      "increment  0.258  loss  1.5704461336135864\n",
      "increment  0.259  loss  1.5720059871673584\n",
      "increment  0.26  loss  1.573554277420044\n",
      "increment  0.261  loss  1.5750901699066162\n",
      "increment  0.262  loss  1.5766161680221558\n",
      "increment  0.263  loss  1.5781238079071045\n",
      "increment  0.264  loss  1.579621434211731\n",
      "increment  0.265  loss  1.5811105966567993\n",
      "increment  0.266  loss  1.5825883150100708\n",
      "increment  0.267  loss  1.5840578079223633\n",
      "increment  0.268  loss  1.5855042934417725\n",
      "increment  0.269  loss  1.5869425535202026\n",
      "increment  0.25  loss  2.36631618277356\n",
      "increment  0.251  loss  2.3662976601734758\n",
      "increment  0.252  loss  2.3662916932195426\n",
      "increment  0.253  loss  2.366297641758248\n",
      "increment  0.254  loss  2.3663158255957066\n",
      "increment  0.255  loss  2.3663459839392456\n",
      "increment  0.256  loss  2.3663878368277103\n",
      "increment  0.257  loss  2.3664412990614774\n",
      "increment  0.258  loss  2.366506535162032\n",
      "increment  0.259  loss  2.3665832415603103\n",
      "increment  0.26  loss  2.3666713567640634\n",
      "increment  0.261  loss  2.366770706907287\n",
      "increment  0.262  loss  2.3668813763368877\n",
      "increment  0.263  loss  2.3670031807582825\n",
      "increment  0.264  loss  2.3671356338731946\n",
      "increment  0.265  loss  2.3672792341306805\n",
      "increment  0.266  loss  2.367433598203212\n",
      "increment  0.267  loss  2.3675987574514004\n",
      "increment  0.268  loss  2.3677743736889214\n",
      "increment  0.269  loss  2.3679606347043065\n",
      "increment  0.25  loss  2.37570312230587\n",
      "increment  0.251  loss  2.3756595760948955\n",
      "increment  0.252  loss  2.3756287662684916\n",
      "increment  0.253  loss  2.3756100524388253\n",
      "increment  0.254  loss  2.3756039050128313\n",
      "increment  0.255  loss  2.375609683259204\n",
      "increment  0.256  loss  2.375627547862753\n",
      "increment  0.257  loss  2.375657558662817\n",
      "increment  0.258  loss  2.375699379833415\n",
      "increment  0.259  loss  2.37575303231813\n",
      "increment  0.26  loss  2.3758182109229264\n",
      "increment  0.261  loss  2.3758950200751423\n",
      "increment  0.262  loss  2.375983315094933\n",
      "increment  0.263  loss  2.3760827996272593\n",
      "increment  0.264  loss  2.3761937069218604\n",
      "increment  0.265  loss  2.376315869611874\n",
      "increment  0.266  loss  2.3764487551137803\n",
      "increment  0.267  loss  2.376592837503925\n",
      "increment  0.268  loss  2.376747854233533\n",
      "increment  0.269  loss  2.3769135878626257\n",
      "savedModel\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-6ae39353b117>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi_epoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0minp_pos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minp_velocity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magentIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_pos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mout_velocity\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msample_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda31\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_profile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    515\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda31\\lib\\site-packages\\torch\\autograd\\profiler.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    615\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_function_enter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-6ae39353b117>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"savedModel\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./FCM_Adam_30Epochs_HDim7k_L2Times10.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "import random\n",
    "import numpy as np\n",
    "n_epochs = 30\n",
    "\n",
    "loss_avg = []\n",
    "rangeArray = []\n",
    "for x in range(20):\n",
    "    rangeArray.append(0.25 + (x * 0.001))\n",
    "    \n",
    "for i in range(len(rangeArray)):\n",
    "    loss_array = []\n",
    "    loss_avg.append(loss_array)\n",
    "\n",
    "for i_epoch in range(n_epochs):\n",
    "    for i_batch, sample_batch in enumerate(train_loader):\n",
    "        inp_pos,inp_velocity, agentIndex, out_pos,out_velocity= sample_batch\n",
    "    #     print(\"inp_velocity\",inp_velocity)\n",
    "    #     print(inp_velocity[:,0])\n",
    "        for i in range(len(rangeArray)):\n",
    "            x_avg_velocity = inp_velocity[0,0]\n",
    "            y_avg_velocity = inp_velocity[0,1]  \n",
    "#                 print(rangeArray[i])\n",
    "            for x in range(1,19):\n",
    "                x_avg_velocity = (x_avg_velocity * (1-rangeArray[i])) + (inp_velocity[x,0] * rangeArray[i])\n",
    "                y_avg_velocity = (y_avg_velocity * (1-rangeArray[i])) + (inp_velocity[x,1] * rangeArray[i])\n",
    "        #     print(\"last position \",inp_pos)\n",
    "            position = numpy.zeros((30,2))\n",
    "            position[0][0] =  x_avg_velocity * 0.1 + inp_pos[-1,0]\n",
    "            position[0][1] =  x_avg_velocity * 0.1 + inp_pos[-1,1]\n",
    "            for x in range(1,30):\n",
    "                position[x][0] = x_avg_velocity * 0.1 + position[x-1][0]\n",
    "                position[x][1] = y_avg_velocity * 0.1 + position[x-1][1]\n",
    "            position = torch.FloatTensor(position)\n",
    "            out_pos = torch.FloatTensor(out_pos)\n",
    "#             print(out)\n",
    "            loss = torch.sqrt((torch.mean((position-out_pos)**2)))\n",
    "\n",
    "            loss_avg[i].append(loss.item())\n",
    "#                 print(rangeArray[i])\n",
    "            if i_batch % 10000 == 0:\n",
    "                print(\"increment \" ,rangeArray[i] , \" loss \",mean(loss_avg[i]))\n",
    "                loss_avg[i] = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10', 'v11', 'v12', 'v13', 'v14', 'v15', 'v16', 'v17', 'v18', 'v19', 'v20', 'v21', 'v22', 'v23', 'v24', 'v25', 'v26', 'v27', 'v28', 'v29', 'v30', 'v31', 'v32', 'v33', 'v34', 'v35', 'v36', 'v37', 'v38', 'v39', 'v40', 'v41', 'v42', 'v43', 'v44', 'v45', 'v46', 'v47', 'v48', 'v49', 'v50', 'v51', 'v52', 'v53', 'v54', 'v55', 'v56', 'v57', 'v58', 'v59', 'v60']\n",
      "batch #:  0\n",
      "batch #:  100\n",
      "batch #:  200\n",
      "batch #:  300\n",
      "batch #:  400\n",
      "batch #:  500\n",
      "batch #:  600\n",
      "batch #:  700\n",
      "batch #:  800\n",
      "batch #:  900\n",
      "batch #:  1000\n",
      "batch #:  1100\n",
      "batch #:  1200\n",
      "batch #:  1300\n",
      "batch #:  1400\n",
      "batch #:  1500\n",
      "batch #:  1600\n",
      "batch #:  1700\n",
      "batch #:  1800\n",
      "batch #:  1900\n",
      "batch #:  2000\n",
      "batch #:  2100\n",
      "batch #:  2200\n",
      "batch #:  2300\n",
      "batch #:  2400\n",
      "batch #:  2500\n",
      "batch #:  2600\n",
      "batch #:  2700\n",
      "batch #:  2800\n",
      "batch #:  2900\n",
      "batch #:  3000\n",
      "batch #:  3100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from statistics import mean\n",
    "# import pandas as pd\n",
    "\n",
    "save_file = \"Physics.csv\"\n",
    "\n",
    "\n",
    "header = [\"ID\"]\n",
    "header += [\"v\"+str(x) for x in range(1, 61)]\n",
    "print(header)\n",
    "\n",
    "with open(save_file, 'w') as f:\n",
    "    f.write(\",\".join(header)+\"\\n\")\n",
    "\n",
    "\n",
    "#         #Changed to 4 because we don't need lane and lane norm\n",
    "batch_sz = 1\n",
    "     \n",
    "for i_batch, sample_batch in enumerate(val_loader):\n",
    "    if i_batch % 100 == 0:\n",
    "        print(\"batch #: \", i_batch * batch_sz)\n",
    "\n",
    "    inp_pos,inp_velocity, scene_idx, agent_ids, track_ids= sample_batch\n",
    "#     print(\"inp_velocity\",inp_velocity)\n",
    "#     print(inp_velocity[:,0])\n",
    "    x_avg_velocity = inp_velocity[0,0]\n",
    "    y_avg_velocity = inp_velocity[0,1]    \n",
    "    for x in range(1,19):\n",
    "        x_avg_velocity = (x_avg_velocity * 0.745) + (inp_velocity[x,0] * 0.255)\n",
    "        y_avg_velocity = (y_avg_velocity * 0.745) + (inp_velocity[x,1] * 0.255)\n",
    "#     print(\"last position \",inp_pos)\n",
    "    position = numpy.zeros((30,2))\n",
    "    position[0][0] =  x_avg_velocity * 0.1 + inp_pos[-1,0]\n",
    "    position[0][1] =  x_avg_velocity * 0.1 + inp_pos[-1,1]\n",
    "    for x in range(1,30):\n",
    "        position[x][0] = x_avg_velocity * 0.1 + position[x-1][0]\n",
    "        position[x][1] = y_avg_velocity * 0.1 + position[x-1][1]\n",
    "#     print(\"position\",position)\n",
    "\n",
    "\n",
    "                           \n",
    " \n",
    "\n",
    "    with open(save_file, \"a\") as f:\n",
    "        row = \"\"\n",
    "        for x in range(len(position) - 1):\n",
    "            row += str(position[x][0]) + \",\" + str(position[x][1]) + \",\"\n",
    "        row += str(position[-1][0]) + \",\" + str(position[-1][1])\n",
    "#             print(scene_idx)\n",
    "        f.write(str(scene_idx[i_scene])+\",\" + row +\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
