{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from statistics import mean,stdev\n",
    "\n",
    "\n",
    "\"\"\"Change to the data folder\"\"\"\n",
    "new_path = \"../new_train/\"\n",
    "val_path = \"../new_val_in/\"\n",
    "\n",
    "# number of sequences in each dataset\n",
    "# train:205942  val:3200 test: 36272 \n",
    "# sequences sampled at 10HZ rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, data_path: str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pkl_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pkl_path = self.pkl_list[idx]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            \n",
    "#         reduce_mem_usage(data)\n",
    "        if self.transform:\n",
    "        \n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "# def reduce_mem_usage(df, verbose=True):\n",
    "#     numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "#     columns = ['p_in','v_in','v_out','lane','lane_norm']\n",
    "#     for col in columns:\n",
    "#             c_min = df[col].min()\n",
    "#             c_max = df[col].max()\n",
    "#             print(\"C_min\",c_min,\" \",np.iinfo(np.int8).min)\n",
    "#             print(\"c_max\",c_max,\" \",np.iinfo(np.int8).max)\n",
    "#             if True:\n",
    "#                 if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "#                     print(\"true\")\n",
    "#                     df[col] = df[col].astype(np.int8)\n",
    "#                 elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "#                     df[col] = df[col].astype(np.int16)\n",
    "#                 elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "#                     df[col] = df[col].astype(np.int32)\n",
    "#                 elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "#                     df[col] = df[col].astype(np.int64)  \n",
    "#             else:\n",
    "#                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "#                     df[col] = df[col].astype(np.float16)\n",
    "#                 elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "#                     df[col] = df[col].astype(np.float32)\n",
    "#                 else:\n",
    "#                     df[col] = df[col].astype(np.float64)    \n",
    "#     return df\n",
    "# intialize a dataset\n",
    "\n",
    "# transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "#                          std=[0.5, 0.5, 0.5])\n",
    "# ])\n",
    "train_dataset  = ArgoverseDataset(data_path=new_path)\n",
    "val_dataset = ArgoverseDataset(data_path=val_path)\n",
    "#print((val_dataset[0]))\n",
    "#print(len(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a loader to enable batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 4\n",
    "\n",
    "def my_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "#     if(len(str(maxVal)) > 3):\n",
    "#         print(len(maxVal))\n",
    "#     print(\"maxStringLength\",len(str(maxVal)))\n",
    "#     print(\"k   \",len(maxVal))\n",
    "    \n",
    "    inp = []\n",
    "    out = []\n",
    "    numbRows = 1000\n",
    "    for scene in batch:\n",
    "#         if len(str(len(scene['lane']))) > 3:\n",
    "#             print(len(scene['lane']))\n",
    "#         print(scene['p_in'])\n",
    "        lanes = numpy.zeros((numbRows,19,3))\n",
    "        lane_norm = numpy.zeros((numbRows,19,3))\n",
    "        pIn = vIn = numpy.zeros((numbRows,19,3))\n",
    "#         lane_norm =[0,0]\n",
    "        lengthLane = min(numbRows,len(scene['lane']))\n",
    "        pIn[:len(scene['p_in']),:,:2] = scene['p_in']\n",
    "        vIn[:len(scene['v_in']),:,:2] = scene['v_in']\n",
    "        lanes[:lengthLane,0,:3] = scene['lane'][:lengthLane,:]\n",
    "        lane_norm[:lengthLane,0,:3] = scene['lane_norm'] [:lengthLane,:]\n",
    "        inp.append(numpy.dstack([pIn,vIn,lanes,lane_norm]))\n",
    "        out.append(numpy.dstack([scene['p_out'], scene['v_out']]))\n",
    "        agent_id= np.array([scene['agent_id'] for scene in batch],dtype=object)\n",
    "        track_id= np.array([scene['track_id'][:,0] for scene in batch])\n",
    "        \n",
    "    inp = torch.sparse(torch.FloatTensor(inp))\n",
    "    out = torch.sparse(torch.FloatTensor(out))\n",
    "        \n",
    "    return [inp, out,numpy.asarray(agent_id),numpy.asarray(track_id)]\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def val_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    inp = [numpy.dstack([scene['p_in'], scene['v_in']]) for scene in batch]\n",
    "    inp = torch.LongTensor(inp)\n",
    "    return inp\n",
    "\n",
    "val_loader = DataLoader(val_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers,dropoutVar=0.2):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = n_layers\n",
    "        self.dropout = nn.Dropout(dropoutVar)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True,dropout=0.5)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    def forward(self, x,previous):\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            device = torch.device(\"cuda:0\")\n",
    "            x = x.to(device)\n",
    "            h0 = 0\n",
    "            c0 = 0\n",
    "            if(previous == 1):\n",
    "                h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim,device=device).requires_grad_()\n",
    "                c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim,device=device).requires_grad_()\n",
    "            else:\n",
    "                hn,cn = previous\n",
    "                h0 = hn\n",
    "                c0 = cn\n",
    "            out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "            out = self.fc(out[:, -1, :]) \n",
    "            \n",
    "#             out = self.dropout(out)  ADD IF OVERFITTING\n",
    "            return out,(hn,cn)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim,device=torch.device(\"cuda:0\"))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_a_histogram(sample_batch, agent_id, xPos, yPos, xVel, yVel):\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    #agent_sz = inp.size(1)\n",
    "    \n",
    "    for i in range(batch_sz):\n",
    "        #hist_data_xPos = np.zeros((60,19));\n",
    "        #hist_data_yPos = np.zeros((60,19));\n",
    "        #hist_data_xVel = np.zeros((60,19));\n",
    "        hist_data_yVel = np.zeros((60,19));\n",
    "        \n",
    "        for j in range(60):\n",
    "            #hist_data_xPos[j] = (inp[i, j,:,0])\n",
    "            #hist_data_yPos[j] = (inp[i, j,:,1])\n",
    "            #hist_data_xVel[j] = (inp[i, j,:,2])\n",
    "            hist_data_yVel[j] = (inp[i, j,:,3])\n",
    "            \n",
    "        for j in range(len(hist_data_yVel)):\n",
    "            for k in range(len(hist_data_yVel[j])):\n",
    "                #xPos.append(hist_data_xPos[j][k])\n",
    "                #yPos.append(hist_data_yPos[j][k])\n",
    "                #xVel.append(hist_data_xVel[j][k])\n",
    "                yVel.append(hist_data_yVel[j][k])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_batch(sample_batch, agent_id):\n",
    "    \"\"\"visualize the trajectory for a batch of samples with a randon agent\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i, agent_id,:,0], inp[i, agent_id,:,1])\n",
    "        axs[i].scatter(out[i, agent_id,:,0], out[i, agent_id,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the batch of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "rowsSeen #:  0  avg loss: 1272.8967134277025\n",
      "rowsSeen #:  40  avg loss: 1035.4652445566653\n",
      "rowsSeen #:  80  avg loss: 1242.164486468633\n",
      "rowsSeen #:  120  avg loss: 1004.4546915501853\n",
      "rowsSeen #:  160  avg loss: 1457.476577326705\n",
      "rowsSeen #:  200  avg loss: 1653.6072672683001\n",
      "rowsSeen #:  240  avg loss: 928.4885604109119\n",
      "rowsSeen #:  280  avg loss: 899.8442958218604\n",
      "rowsSeen #:  320  avg loss: 868.9512193649914\n",
      "rowsSeen #:  360  avg loss: 1012.308119630913\n",
      "rowsSeen #:  400  avg loss: 665.5177795279462\n",
      "rowsSeen #:  440  avg loss: 701.2004108149434\n",
      "rowsSeen #:  480  avg loss: 711.8553521788082\n",
      "rowsSeen #:  520  avg loss: 1184.820278878975\n",
      "rowsSeen #:  560  avg loss: 973.4908709893344\n",
      "rowsSeen #:  600  avg loss: 1047.0313925888963\n",
      "rowsSeen #:  640  avg loss: 959.7693649898845\n",
      "rowsSeen #:  680  avg loss: 924.7784717907718\n",
      "rowsSeen #:  720  avg loss: 1126.9705387735796\n",
      "rowsSeen #:  760  avg loss: 906.8472390528925\n",
      "rowsSeen #:  800  avg loss: 979.3896255665583\n",
      "rowsSeen #:  840  avg loss: 961.6147543745426\n",
      "rowsSeen #:  880  avg loss: 1019.5910012355554\n",
      "rowsSeen #:  920  avg loss: 985.9738904934625\n",
      "rowsSeen #:  960  avg loss: 1128.8449921730532\n",
      "rowsSeen #:  1000  avg loss: 731.0388710499437\n",
      "rowsSeen #:  1040  avg loss: 1390.3822544824084\n",
      "rowsSeen #:  1080  avg loss: 1141.6847498163334\n",
      "rowsSeen #:  1120  avg loss: 869.9449889449775\n",
      "rowsSeen #:  1160  avg loss: 822.0963514584448\n",
      "rowsSeen #:  1200  avg loss: 989.6076894907156\n",
      "rowsSeen #:  1240  avg loss: 1042.9279497520376\n",
      "rowsSeen #:  1280  avg loss: 868.995109842519\n",
      "rowsSeen #:  1320  avg loss: 828.8743923555066\n",
      "rowsSeen #:  1360  avg loss: 675.1724197755009\n",
      "rowsSeen #:  1400  avg loss: 939.8473246210721\n",
      "rowsSeen #:  1440  avg loss: 743.2868616255124\n",
      "rowsSeen #:  1480  avg loss: 942.2940329067285\n",
      "rowsSeen #:  1520  avg loss: 942.8358550353348\n",
      "rowsSeen #:  1560  avg loss: 1146.8436517237624\n",
      "rowsSeen #:  1600  avg loss: 818.0891489384571\n",
      "rowsSeen #:  1640  avg loss: 1103.77315789963\n",
      "rowsSeen #:  1680  avg loss: 838.4866416959294\n",
      "rowsSeen #:  1720  avg loss: 1037.9882108000447\n",
      "rowsSeen #:  1760  avg loss: 912.9805068288247\n",
      "rowsSeen #:  1800  avg loss: 639.4285149153951\n",
      "rowsSeen #:  1840  avg loss: 724.1284693341438\n",
      "rowsSeen #:  1880  avg loss: 920.4824781553863\n",
      "rowsSeen #:  1920  avg loss: 897.9699663113306\n",
      "rowsSeen #:  1960  avg loss: 926.0453573478262\n",
      "rowsSeen #:  2000  avg loss: 722.3813916441692\n",
      "rowsSeen #:  2040  avg loss: 965.2663979210535\n",
      "rowsSeen #:  2080  avg loss: 659.1727186271565\n",
      "rowsSeen #:  2120  avg loss: 1048.787030841882\n",
      "rowsSeen #:  2160  avg loss: 883.1681754782796\n",
      "rowsSeen #:  2200  avg loss: 924.1782648955484\n",
      "rowsSeen #:  2240  avg loss: 944.0680078526462\n",
      "rowsSeen #:  2280  avg loss: 629.6269545736288\n",
      "rowsSeen #:  2320  avg loss: 848.3962324707311\n",
      "rowsSeen #:  2360  avg loss: 525.18376173303\n",
      "rowsSeen #:  2400  avg loss: 994.6268480119171\n",
      "rowsSeen #:  2440  avg loss: 960.4760184377432\n",
      "rowsSeen #:  2480  avg loss: 540.2583771154099\n",
      "rowsSeen #:  2520  avg loss: 907.945468174126\n",
      "rowsSeen #:  2560  avg loss: 926.4126867865709\n",
      "rowsSeen #:  2600  avg loss: 664.42987332946\n",
      "rowsSeen #:  2640  avg loss: 707.8737632077188\n",
      "rowsSeen #:  2680  avg loss: 1035.4257688667672\n",
      "rowsSeen #:  2720  avg loss: 821.9261077523176\n",
      "rowsSeen #:  2760  avg loss: 812.6500339709341\n",
      "rowsSeen #:  2800  avg loss: 929.67226997354\n",
      "rowsSeen #:  2840  avg loss: 790.0369516390358\n",
      "rowsSeen #:  2880  avg loss: 811.7227205950735\n",
      "rowsSeen #:  2920  avg loss: 957.2272527527685\n",
      "rowsSeen #:  2960  avg loss: 880.02898496565\n",
      "rowsSeen #:  3000  avg loss: 643.7491817286983\n",
      "rowsSeen #:  3040  avg loss: 1038.5155074524134\n",
      "rowsSeen #:  3080  avg loss: 746.3357859163732\n",
      "rowsSeen #:  3120  avg loss: 865.5234050288898\n",
      "rowsSeen #:  3160  avg loss: 961.5796229470272\n",
      "rowsSeen #:  3200  avg loss: 917.1761078308026\n",
      "rowsSeen #:  3240  avg loss: 825.9062496059885\n",
      "rowsSeen #:  3280  avg loss: 1129.7869248928625\n",
      "rowsSeen #:  3320  avg loss: 865.7798324211439\n",
      "rowsSeen #:  3360  avg loss: 873.8521842324734\n",
      "rowsSeen #:  3400  avg loss: 711.7511323758284\n",
      "rowsSeen #:  3440  avg loss: 910.0209431218852\n",
      "rowsSeen #:  3480  avg loss: 1092.1747795954304\n",
      "rowsSeen #:  3520  avg loss: 490.5946654031092\n",
      "rowsSeen #:  3560  avg loss: 500.1802858880517\n",
      "rowsSeen #:  3600  avg loss: 964.7564446615676\n",
      "rowsSeen #:  3640  avg loss: 852.3538746619116\n",
      "rowsSeen #:  3680  avg loss: 722.2804987629813\n",
      "rowsSeen #:  3720  avg loss: 1225.0746557126217\n",
      "rowsSeen #:  3760  avg loss: 798.1363406058152\n",
      "rowsSeen #:  3800  avg loss: 728.7148100780696\n",
      "rowsSeen #:  3840  avg loss: 956.4131718720744\n",
      "rowsSeen #:  3880  avg loss: 754.6064723232388\n",
      "rowsSeen #:  3920  avg loss: 738.980308729435\n",
      "rowsSeen #:  3960  avg loss: 851.3877883008635\n",
      "rowsSeen #:  4000  avg loss: 698.2287886580825\n",
      "rowsSeen #:  4040  avg loss: 940.1347108240923\n",
      "rowsSeen #:  4080  avg loss: 784.7249722332643\n",
      "rowsSeen #:  4120  avg loss: 763.3499572573726\n",
      "rowsSeen #:  4160  avg loss: 693.4689308013022\n",
      "rowsSeen #:  4200  avg loss: 527.8123132767\n",
      "rowsSeen #:  4240  avg loss: 579.3170650696134\n",
      "rowsSeen #:  4280  avg loss: 796.5681977945194\n",
      "rowsSeen #:  4320  avg loss: 749.0525355315457\n",
      "rowsSeen #:  4360  avg loss: 933.4896754107566\n",
      "rowsSeen #:  4400  avg loss: 802.4827793839271\n",
      "rowsSeen #:  4440  avg loss: 962.4060561158245\n",
      "rowsSeen #:  4480  avg loss: 973.3643411227254\n",
      "rowsSeen #:  4520  avg loss: 551.0207180582003\n",
      "rowsSeen #:  4560  avg loss: 905.5938809822195\n",
      "rowsSeen #:  4600  avg loss: 895.9566857157151\n",
      "rowsSeen #:  4640  avg loss: 863.2305594026241\n",
      "rowsSeen #:  4680  avg loss: 744.3013963627749\n",
      "rowsSeen #:  4720  avg loss: 614.8338823223984\n",
      "rowsSeen #:  4760  avg loss: 636.8689542817452\n",
      "rowsSeen #:  4800  avg loss: 899.2722338579098\n",
      "rowsSeen #:  4840  avg loss: 544.6875931949913\n",
      "rowsSeen #:  4880  avg loss: 751.7286390931133\n",
      "rowsSeen #:  4920  avg loss: 733.4249236699087\n",
      "rowsSeen #:  4960  avg loss: 904.1226583120192\n",
      "rowsSeen #:  5000  avg loss: 922.8817085336646\n",
      "rowsSeen #:  5040  avg loss: 594.14654649027\n",
      "rowsSeen #:  5080  avg loss: 934.2608252053294\n",
      "rowsSeen #:  5120  avg loss: 676.4738028730031\n",
      "rowsSeen #:  5160  avg loss: 774.9069411380092\n",
      "rowsSeen #:  5200  avg loss: 900.0297238842968\n",
      "rowsSeen #:  5240  avg loss: 838.6005888478298\n",
      "rowsSeen #:  5280  avg loss: 899.0335385261225\n",
      "rowsSeen #:  5320  avg loss: 870.2205980312306\n",
      "rowsSeen #:  5360  avg loss: 853.1301469588279\n",
      "rowsSeen #:  5400  avg loss: 771.3384016257703\n",
      "rowsSeen #:  5440  avg loss: 1025.8825695195362\n",
      "rowsSeen #:  5480  avg loss: 558.4418088778016\n",
      "rowsSeen #:  5520  avg loss: 667.0557898110362\n",
      "rowsSeen #:  5560  avg loss: 669.5279677708813\n",
      "rowsSeen #:  5600  avg loss: 769.786312764585\n",
      "rowsSeen #:  5640  avg loss: 989.5154524958134\n",
      "rowsSeen #:  5680  avg loss: 745.2629241915171\n",
      "rowsSeen #:  5720  avg loss: 657.8468830932428\n",
      "rowsSeen #:  5760  avg loss: 913.1898605753978\n",
      "rowsSeen #:  5800  avg loss: 1038.3950626922656\n",
      "rowsSeen #:  5840  avg loss: 1124.4662544491143\n",
      "rowsSeen #:  5880  avg loss: 941.7582667666921\n",
      "rowsSeen #:  5920  avg loss: 880.5916004348223\n",
      "rowsSeen #:  5960  avg loss: 870.2675476290286\n",
      "rowsSeen #:  6000  avg loss: 720.4001487052208\n",
      "rowsSeen #:  6040  avg loss: 843.7323636726725\n",
      "rowsSeen #:  6080  avg loss: 771.4264927975834\n",
      "rowsSeen #:  6120  avg loss: 1123.4803249596805\n",
      "rowsSeen #:  6160  avg loss: 974.0095361321419\n",
      "rowsSeen #:  6200  avg loss: 789.6398946600656\n",
      "rowsSeen #:  6240  avg loss: 950.2656696193914\n",
      "rowsSeen #:  6280  avg loss: 692.1258111745944\n",
      "rowsSeen #:  6320  avg loss: 670.5208066809178\n",
      "rowsSeen #:  6360  avg loss: 828.5596749517819\n",
      "rowsSeen #:  6400  avg loss: 487.0969351211439\n",
      "rowsSeen #:  6440  avg loss: 652.9922292445307\n",
      "rowsSeen #:  6480  avg loss: 553.8143102229138\n",
      "rowsSeen #:  6520  avg loss: 649.7118114719789\n",
      "rowsSeen #:  6560  avg loss: 817.6956884683668\n",
      "rowsSeen #:  6600  avg loss: 926.4069622093315\n",
      "rowsSeen #:  6640  avg loss: 639.9458198958613\n",
      "rowsSeen #:  6680  avg loss: 990.338700472402\n",
      "rowsSeen #:  6720  avg loss: 619.2443903974195\n",
      "rowsSeen #:  6760  avg loss: 706.2343817633887\n",
      "rowsSeen #:  6800  avg loss: 754.5402687206943\n",
      "rowsSeen #:  6840  avg loss: 860.5921122861902\n",
      "rowsSeen #:  6880  avg loss: 742.7391738246878\n",
      "rowsSeen #:  6920  avg loss: 632.4981687993602\n",
      "rowsSeen #:  6960  avg loss: 523.867938422915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rowsSeen #:  7000  avg loss: 731.3182914061969\n",
      "rowsSeen #:  7040  avg loss: 700.1863346724201\n",
      "rowsSeen #:  7080  avg loss: 581.8241695577154\n",
      "rowsSeen #:  7120  avg loss: 480.5255182314757\n",
      "rowsSeen #:  7160  avg loss: 930.9731729666391\n",
      "rowsSeen #:  7200  avg loss: 804.6898390885318\n",
      "rowsSeen #:  7240  avg loss: 594.9150288103397\n",
      "rowsSeen #:  7280  avg loss: 761.4298697069288\n",
      "rowsSeen #:  7320  avg loss: 547.5769667260969\n",
      "rowsSeen #:  7360  avg loss: 698.5507729133033\n",
      "rowsSeen #:  7400  avg loss: 1075.600728502497\n",
      "rowsSeen #:  7440  avg loss: 532.2791401253703\n",
      "rowsSeen #:  7480  avg loss: 946.2446402196834\n",
      "rowsSeen #:  7520  avg loss: 486.3313694216994\n",
      "rowsSeen #:  7560  avg loss: 918.9290096339374\n",
      "rowsSeen #:  7600  avg loss: 727.7429943418648\n",
      "rowsSeen #:  7640  avg loss: 674.8909095731648\n",
      "rowsSeen #:  7680  avg loss: 683.3149171889067\n",
      "rowsSeen #:  7720  avg loss: 754.1170167874669\n",
      "rowsSeen #:  7760  avg loss: 631.4553576591859\n",
      "rowsSeen #:  7800  avg loss: 726.8831010436378\n",
      "rowsSeen #:  7840  avg loss: 622.5439502902483\n",
      "rowsSeen #:  7880  avg loss: 634.2916322779904\n",
      "rowsSeen #:  7920  avg loss: 681.7288821287198\n",
      "rowsSeen #:  7960  avg loss: 599.5537963576091\n",
      "rowsSeen #:  8000  avg loss: 731.645410071356\n",
      "rowsSeen #:  8040  avg loss: 738.5241715457353\n",
      "rowsSeen #:  8080  avg loss: 820.9376443711956\n",
      "rowsSeen #:  8120  avg loss: 374.9215682504621\n",
      "rowsSeen #:  8160  avg loss: 584.7052972586474\n",
      "rowsSeen #:  8200  avg loss: 589.2721483295518\n",
      "rowsSeen #:  8240  avg loss: 561.7776489040544\n",
      "rowsSeen #:  8280  avg loss: 550.2650774469732\n",
      "rowsSeen #:  8320  avg loss: 824.1552749274019\n",
      "rowsSeen #:  8360  avg loss: 765.8986426531111\n",
      "rowsSeen #:  8400  avg loss: 642.180345270384\n",
      "rowsSeen #:  8440  avg loss: 671.7455011609275\n",
      "rowsSeen #:  8480  avg loss: 739.2076897751167\n",
      "rowsSeen #:  8520  avg loss: 398.8666905824747\n",
      "rowsSeen #:  8560  avg loss: 599.3131992353913\n",
      "rowsSeen #:  8600  avg loss: 568.9299160882116\n",
      "rowsSeen #:  8640  avg loss: 672.9640432184927\n",
      "rowsSeen #:  8680  avg loss: 826.744876699654\n",
      "rowsSeen #:  8720  avg loss: 670.7888490613698\n",
      "rowsSeen #:  8760  avg loss: 736.4074270618222\n",
      "rowsSeen #:  8800  avg loss: 461.82018025747846\n",
      "rowsSeen #:  8840  avg loss: 596.7421272279341\n",
      "rowsSeen #:  8880  avg loss: 944.406702489946\n",
      "rowsSeen #:  8920  avg loss: 641.6412491933887\n",
      "rowsSeen #:  8960  avg loss: 936.0044187228044\n",
      "rowsSeen #:  9000  avg loss: 916.0354398517311\n",
      "rowsSeen #:  9040  avg loss: 778.8323524839036\n",
      "rowsSeen #:  9080  avg loss: 712.3887268225266\n",
      "rowsSeen #:  9120  avg loss: 600.7684630635675\n",
      "rowsSeen #:  9160  avg loss: 812.447218005916\n",
      "rowsSeen #:  9200  avg loss: 760.9425783501038\n",
      "rowsSeen #:  9240  avg loss: 975.2157201198318\n",
      "rowsSeen #:  9280  avg loss: 894.7213729547528\n",
      "rowsSeen #:  9320  avg loss: 1056.8614705998898\n",
      "rowsSeen #:  9360  avg loss: 705.2301533793149\n",
      "rowsSeen #:  9400  avg loss: 676.0505506609968\n",
      "rowsSeen #:  9440  avg loss: 836.9621009469452\n",
      "rowsSeen #:  9480  avg loss: 900.2621025706852\n",
      "rowsSeen #:  9520  avg loss: 319.5904758847446\n",
      "rowsSeen #:  9560  avg loss: 761.6069524335861\n",
      "rowsSeen #:  9600  avg loss: 790.0347532971364\n",
      "rowsSeen #:  9640  avg loss: 430.7678058557315\n",
      "rowsSeen #:  9680  avg loss: 762.725442876017\n",
      "rowsSeen #:  9720  avg loss: 434.3832877771564\n",
      "rowsSeen #:  9760  avg loss: 637.103988962487\n",
      "rowsSeen #:  9800  avg loss: 709.2124390919391\n",
      "rowsSeen #:  9840  avg loss: 1056.0174361599993\n",
      "rowsSeen #:  9880  avg loss: 826.7961243102804\n",
      "rowsSeen #:  9920  avg loss: 808.2546124837412\n",
      "rowsSeen #:  9960  avg loss: 628.8653000300191\n",
      "rowsSeen #:  10000  avg loss: 861.511103299439\n",
      "rowsSeen #:  10040  avg loss: 531.1232851932978\n",
      "rowsSeen #:  10080  avg loss: 760.4935818684834\n",
      "rowsSeen #:  10120  avg loss: 777.249648781266\n",
      "rowsSeen #:  10160  avg loss: 773.2924592116418\n",
      "rowsSeen #:  10200  avg loss: 747.8107181004117\n",
      "rowsSeen #:  10240  avg loss: 771.1314569046418\n",
      "rowsSeen #:  10280  avg loss: 670.5188820875436\n",
      "rowsSeen #:  10320  avg loss: 666.9950969734217\n",
      "rowsSeen #:  10360  avg loss: 694.2671427306099\n",
      "rowsSeen #:  10400  avg loss: 504.7129972048255\n",
      "rowsSeen #:  10440  avg loss: 855.6637674282041\n",
      "rowsSeen #:  10480  avg loss: 689.1962290879494\n",
      "rowsSeen #:  10520  avg loss: 606.0147226539126\n",
      "rowsSeen #:  10560  avg loss: 813.1576448093172\n",
      "rowsSeen #:  10600  avg loss: 576.3102000384157\n",
      "rowsSeen #:  10640  avg loss: 701.7945504517216\n",
      "rowsSeen #:  10680  avg loss: 996.5726597408764\n",
      "rowsSeen #:  10720  avg loss: 846.2816412812937\n",
      "rowsSeen #:  10760  avg loss: 642.8980003809556\n",
      "rowsSeen #:  10800  avg loss: 468.41005998443813\n",
      "rowsSeen #:  10840  avg loss: 692.6342000320336\n",
      "rowsSeen #:  10880  avg loss: 716.8970414368932\n",
      "rowsSeen #:  10920  avg loss: 728.8553558858235\n",
      "rowsSeen #:  10960  avg loss: 622.3524861066417\n",
      "rowsSeen #:  11000  avg loss: 649.8334551510711\n",
      "rowsSeen #:  11040  avg loss: 597.3605230245863\n",
      "rowsSeen #:  11080  avg loss: 636.7170238501703\n",
      "rowsSeen #:  11120  avg loss: 659.8791355793142\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "agent_id = 0\n",
    "learning_rate = 1\n",
    "momentum = 0.1\n",
    "device = torch.device(\"cuda:0\")\n",
    "input_dim = 12    # input dimension\n",
    "hidden_dim = 12  # hidden layer dimension\n",
    "layer_dim = 10     # number of hidden layers\n",
    "output_dim = 4   # output dimension\n",
    "\n",
    "n_epochs = 5\n",
    "lr=0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "#model = RNNModel(input_dim, output_dim, hidden_dim, layer_dim).to(device)\n",
    "model = RNNModel(input_size=input_dim, output_size=output_dim, hidden_dim=hidden_dim, n_layers=layer_dim)\n",
    "model = model.to(device)\n",
    "# model = DataParallel(model,device)\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate,lr_decay=0.00000001)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    \n",
    "model.train()\n",
    "import time\n",
    "print(\"test\")\n",
    "newOut = torch.zeros((batch_sz,1000,30,4))\n",
    "batch = []\n",
    "timeTotalLoss = 0\n",
    "timeTotalModel = 0\n",
    "timeEnumeration = 0\n",
    "initHidden = 1\n",
    "def my_loss(output, target,batch_num):\n",
    "    loss = torch.mean((output[batch_num,:60] - target)**2)\n",
    "    return loss\n",
    "\n",
    "for i_epoch in range(n_epochs):\n",
    "    timeStart = time.time()\n",
    "    for i_batch, sample_batch in enumerate(train_loader):\n",
    "        timeEnumeration = time.time() - timeStart\n",
    "        timeStart = time.time()\n",
    "    #     print(\"test\")\n",
    "        inp, out,agent_id,track_id = sample_batch\n",
    "#         print(inp.shape)\n",
    "        optimizer.zero_grad()\n",
    "        scaled_loss = 0\n",
    "        newOut[:,:60,:,:] = out\n",
    "        for j in range(batch_sz):\n",
    "#             print(\"agent_id\",agent_id)\n",
    "#             print(\"track_id\",track_id)\n",
    "            agentIndex = numpy.where(track_id[j]==agent_id[j])\n",
    "            for i in range(30):\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    timeStart = time.time()\n",
    "                    output,hidden = model(inp[j].float().cuda(),1)\n",
    "                    timeTotalModel += time.time() - timeStart\n",
    "                    \n",
    "                initHidden = hidden \n",
    "                hn,cn = initHidden\n",
    "                \n",
    "                \n",
    "                x = inp[j,:60,:,:4]\n",
    "                x = torch.roll(x,-1,dims=1)\n",
    "                inp[j,:60,:19,:4] = x\n",
    "                inp[j,:60,18,:4] = out[j,:60,i,:]\n",
    "                \n",
    "                x = output[agentIndex]\n",
    "                output[:,:] = newOut[j,:,i,:]\n",
    "#                 print(output)\n",
    "                output[agentIndex]= x\n",
    "                \n",
    "                timeStart = time.time()\n",
    "                loss = nn.MSELoss()\n",
    "                loss = loss(output.cuda(),newOut[j,:,i,:].cuda())\n",
    "#                 print(loss)\n",
    "                scaler.scale(loss).backward(retain_graph=True)\n",
    "                scaled_loss += loss.item()\n",
    "                timeTotalLoss = time.time() - timeStart\n",
    "        scaler.step(optimizer)\n",
    "        batch.append(scaled_loss/(30 * batch_sz))\n",
    "        scaler.update()\n",
    "#         print(\"timeEnumeration\",timeEnumeration)\n",
    "#         print(\"timeTotalLoss \",timeTotalLoss)\n",
    "#         print(\"timeTotalModel \",timeTotalModel)\n",
    "        timeTotalLoss = timeTotalModel = 0\n",
    "        if i_batch % math.floor(10) == 0:\n",
    "            print(\"rowsSeen #: \",i_batch * batch_sz,\" avg loss:\",mean(batch))\n",
    "            batch = []\n",
    "\n",
    "# from statistics import mean\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import math\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# agent_id = 0\n",
    "# learning_rate = 1\n",
    "# momentum = 0.1\n",
    "# device = torch.device(\"cuda:0\")\n",
    "# input_dim = 12    # input dimension\n",
    "# hidden_dim = 1  # hidden layer dimension\n",
    "# layer_dim = 10     # number of hidden layers\n",
    "# output_dim = 4   # output dimension\n",
    "\n",
    "# n_epochs = 5\n",
    "# lr=0.01\n",
    "\n",
    "# # Define Loss, Optimizer\n",
    "# #model = RNNModel(input_dim, output_dim, hidden_dim, layer_dim).to(device)\n",
    "# model = RNNModel(input_size=input_dim, output_size=output_dim, hidden_dim=12, n_layers=1)\n",
    "# model = model.to(device)\n",
    "# # model = DataParallel(model,device)\n",
    "# optimizer = optim.Adagrad(model.parameters(), lr=learning_rate,lr_decay=0.00000001)\n",
    "# scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    \n",
    "# model.train()\n",
    "\n",
    "# print(\"test\")\n",
    "# newOut = torch.zeros((batch_sz,100,30,4))\n",
    "# batch = []\n",
    "# for i_epoch in range(n_epochs):\n",
    "#     for i_batch, sample_batch in enumerate(train_loader):\n",
    "#     #     print(\"test\")\n",
    "#         inp, out = sample_batch\n",
    "# #         print(inp[0])\n",
    "#         optimizer.zero_grad()\n",
    "#         initHidden = 1\n",
    "#         scaled_loss = 0\n",
    "#         newOut[:,:60,:,:] = out\n",
    "#         for j in range(batch_sz):\n",
    "#             for i in range(30):\n",
    "\n",
    "#                 with torch.cuda.amp.autocast():\n",
    "#                     output,hidden = model(inp[j].float().cuda(),initHidden)\n",
    "\n",
    "#                 initHidden = hidden\n",
    "#                 hn,cn = initHidden\n",
    "\n",
    "# #                 import random\n",
    "# #                 x = random.uniform(0, 1)\n",
    "# #                 if x < 0.5:\n",
    "# #                     continue\n",
    "#                 loss = nn.MSELoss()\n",
    "#                 loss = loss(output.cuda(),newOut[j,:,i,:].cuda())\n",
    "#                 scaler.scale(loss.to(torch.float16)).backward(retain_graph=True)\n",
    "#                 scaled_loss += loss.item()\n",
    "#         scaler.step(optimizer)\n",
    "#         batch.append(scaled_loss/(30 * batch_sz))\n",
    "#         scaler.update()\n",
    "#         if i_batch % math.floor(100) == 0:\n",
    "#             print(\"rowsSeen #: \",i_batch * batch_sz,\" avg loss (past 100): \",mean(batch))\n",
    "#             batch = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
