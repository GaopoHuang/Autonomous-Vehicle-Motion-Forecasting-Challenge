{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from statistics import mean,stdev\n",
    "\n",
    "\n",
    "\"\"\"Change to the data folder\"\"\"\n",
    "new_path = \"../new_train/\"\n",
    "val_path = \"../new_val_in/\"\n",
    "\n",
    "# number of sequences in each dataset\n",
    "# train:205942  val:3200 test: 36272 \n",
    "# sequences sampled at 10HZ rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, data_path: str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pkl_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pkl_path = self.pkl_list[idx]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            \n",
    "#         reduce_mem_usage(data)\n",
    "        if self.transform:\n",
    "        \n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "# def reduce_mem_usage(df, verbose=True):\n",
    "#     numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "#     columns = ['p_in','v_in','v_out','lane','lane_norm']\n",
    "#     for col in columns:\n",
    "#             c_min = df[col].min()\n",
    "#             c_max = df[col].max()\n",
    "#             print(\"C_min\",c_min,\" \",np.iinfo(np.int8).min)\n",
    "#             print(\"c_max\",c_max,\" \",np.iinfo(np.int8).max)\n",
    "#             if True:\n",
    "#                 if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "#                     print(\"true\")\n",
    "#                     df[col] = df[col].astype(np.int8)\n",
    "#                 elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "#                     df[col] = df[col].astype(np.int16)\n",
    "#                 elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "#                     df[col] = df[col].astype(np.int32)\n",
    "#                 elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "#                     df[col] = df[col].astype(np.int64)  \n",
    "#             else:\n",
    "#                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "#                     df[col] = df[col].astype(np.float16)\n",
    "#                 elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "#                     df[col] = df[col].astype(np.float32)\n",
    "#                 else:\n",
    "#                     df[col] = df[col].astype(np.float64)    \n",
    "#     return df\n",
    "# intialize a dataset\n",
    "\n",
    "# transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "#                          std=[0.5, 0.5, 0.5])\n",
    "# ])\n",
    "train_dataset  = ArgoverseDataset(data_path=new_path)\n",
    "val_dataset = ArgoverseDataset(data_path=val_path)\n",
    "#print((val_dataset[0]))\n",
    "#print(len(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a loader to enable batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 4\n",
    "\n",
    "def my_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "#     if(len(str(maxVal)) > 3):\n",
    "#         print(len(maxVal))\n",
    "#     print(\"maxStringLength\",len(str(maxVal)))\n",
    "#     print(\"k   \",len(maxVal))\n",
    "    \n",
    "    inp = []\n",
    "    out = []\n",
    "    numbRows = 1000\n",
    "    for scene in batch:\n",
    "#         if len(str(len(scene['lane']))) > 3:\n",
    "#             print(len(scene['lane']))\n",
    "#         print(scene['p_in'])\n",
    "        lanes = numpy.zeros((numbRows,19,3))\n",
    "        lane_norm = numpy.zeros((numbRows,19,3))\n",
    "        pIn = vIn = numpy.zeros((numbRows,19,3))\n",
    "#         lane_norm =[0,0]\n",
    "        lengthLane = min(numbRows,len(scene['lane']))\n",
    "        pIn[:len(scene['p_in']),:,:2] = scene['p_in']\n",
    "        vIn[:len(scene['v_in']),:,:2] = scene['v_in']\n",
    "        lanes[:lengthLane,0,:3] = scene['lane'][:lengthLane,:]\n",
    "        lane_norm[:lengthLane,0,:3] = scene['lane_norm'] [:lengthLane,:]\n",
    "        inp.append(numpy.dstack([pIn,vIn,lanes,lane_norm]))\n",
    "        out.append(numpy.dstack([scene['p_out'], scene['v_out']]))\n",
    "        agent_id= np.array([scene['agent_id'] for scene in batch],dtype=object)\n",
    "        track_id= np.array([scene['track_id'][:,0] for scene in batch])\n",
    "        \n",
    "    inp = torch.sparse(torch.FloatTensor(inp))\n",
    "    out = torch.sparse(torch.FloatTensor(out))\n",
    "        \n",
    "    return [inp, out,numpy.asarray(agent_id),numpy.asarray(track_id)]\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def val_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    inp = [numpy.dstack([scene['p_in'], scene['v_in']]) for scene in batch]\n",
    "    inp = torch.LongTensor(inp)\n",
    "    return inp\n",
    "\n",
    "val_loader = DataLoader(val_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers,dropoutVar=0.2):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = n_layers\n",
    "        self.dropout = nn.Dropout(dropoutVar)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True,dropout=0.5)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    def forward(self, x,previous):\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            device = torch.device(\"cuda:0\")\n",
    "            x = x.to(device)\n",
    "            h0 = 0\n",
    "            c0 = 0\n",
    "            if(previous == 1):\n",
    "                h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim,device=device).requires_grad_()\n",
    "                c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim,device=device).requires_grad_()\n",
    "            else:\n",
    "                hn,cn = previous\n",
    "                h0 = hn\n",
    "                c0 = cn\n",
    "            out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "            out = self.fc(out[:, -1, :]) \n",
    "            \n",
    "#             out = self.dropout(out)  ADD IF OVERFITTING\n",
    "            return out,(hn,cn)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim,device=torch.device(\"cuda:0\"))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_a_histogram(sample_batch, agent_id, xPos, yPos, xVel, yVel):\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    #agent_sz = inp.size(1)\n",
    "    \n",
    "    for i in range(batch_sz):\n",
    "        #hist_data_xPos = np.zeros((60,19));\n",
    "        #hist_data_yPos = np.zeros((60,19));\n",
    "        #hist_data_xVel = np.zeros((60,19));\n",
    "        hist_data_yVel = np.zeros((60,19));\n",
    "        \n",
    "        for j in range(60):\n",
    "            #hist_data_xPos[j] = (inp[i, j,:,0])\n",
    "            #hist_data_yPos[j] = (inp[i, j,:,1])\n",
    "            #hist_data_xVel[j] = (inp[i, j,:,2])\n",
    "            hist_data_yVel[j] = (inp[i, j,:,3])\n",
    "            \n",
    "        for j in range(len(hist_data_yVel)):\n",
    "            for k in range(len(hist_data_yVel[j])):\n",
    "                #xPos.append(hist_data_xPos[j][k])\n",
    "                #yPos.append(hist_data_yPos[j][k])\n",
    "                #xVel.append(hist_data_xVel[j][k])\n",
    "                yVel.append(hist_data_yVel[j][k])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_batch(sample_batch, agent_id):\n",
    "    \"\"\"visualize the trajectory for a batch of samples with a randon agent\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i, agent_id,:,0], inp[i, agent_id,:,1])\n",
    "        axs[i].scatter(out[i, agent_id,:,0], out[i, agent_id,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the batch of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "rowsSeen #:  0  avg loss: 1272.8967134277025\n",
      "rowsSeen #:  40  avg loss: 1035.4652445566653\n",
      "rowsSeen #:  80  avg loss: 1242.164486468633\n",
      "rowsSeen #:  120  avg loss: 1004.4546915501853\n",
      "rowsSeen #:  160  avg loss: 1457.476577326705\n",
      "rowsSeen #:  200  avg loss: 1653.6072672683001\n",
      "rowsSeen #:  240  avg loss: 928.4885604109119\n",
      "rowsSeen #:  280  avg loss: 899.8442958218604\n",
      "rowsSeen #:  320  avg loss: 868.9512193649914\n",
      "rowsSeen #:  360  avg loss: 1012.308119630913\n",
      "rowsSeen #:  400  avg loss: 665.5177795279462\n",
      "rowsSeen #:  440  avg loss: 701.2004108149434\n",
      "rowsSeen #:  480  avg loss: 711.8553521788082\n",
      "rowsSeen #:  520  avg loss: 1184.820278878975\n",
      "rowsSeen #:  560  avg loss: 973.4908709893344\n",
      "rowsSeen #:  600  avg loss: 1047.0313925888963\n",
      "rowsSeen #:  640  avg loss: 959.7693649898845\n",
      "rowsSeen #:  680  avg loss: 924.7784717907718\n",
      "rowsSeen #:  720  avg loss: 1126.9705387735796\n",
      "rowsSeen #:  760  avg loss: 906.8472390528925\n",
      "rowsSeen #:  800  avg loss: 979.3896255665583\n",
      "rowsSeen #:  840  avg loss: 961.6147543745426\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "agent_id = 0\n",
    "learning_rate = 1\n",
    "momentum = 0.1\n",
    "device = torch.device(\"cuda:0\")\n",
    "input_dim = 12    # input dimension\n",
    "hidden_dim = 12  # hidden layer dimension\n",
    "layer_dim = 10     # number of hidden layers\n",
    "output_dim = 4   # output dimension\n",
    "\n",
    "n_epochs = 5\n",
    "lr=0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "#model = RNNModel(input_dim, output_dim, hidden_dim, layer_dim).to(device)\n",
    "model = RNNModel(input_size=input_dim, output_size=output_dim, hidden_dim=hidden_dim, n_layers=layer_dim)\n",
    "model = model.to(device)\n",
    "# model = DataParallel(model,device)\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate,lr_decay=0.00000001)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    \n",
    "model.train()\n",
    "import time\n",
    "print(\"test\")\n",
    "newOut = torch.zeros((batch_sz,1000,30,4))\n",
    "batch = []\n",
    "timeTotalLoss = 0\n",
    "timeTotalModel = 0\n",
    "timeEnumeration = 0\n",
    "initHidden = 1\n",
    "def my_loss(output, target,batch_num):\n",
    "    loss = torch.mean((output[batch_num,:60] - target)**2)\n",
    "    return loss\n",
    "\n",
    "for i_epoch in range(n_epochs):\n",
    "    timeStart = time.time()\n",
    "    for i_batch, sample_batch in enumerate(train_loader):\n",
    "        timeEnumeration = time.time() - timeStart\n",
    "        timeStart = time.time()\n",
    "    #     print(\"test\")\n",
    "        inp, out,agent_id,track_id = sample_batch\n",
    "#         print(inp.shape)\n",
    "        optimizer.zero_grad()\n",
    "        scaled_loss = 0\n",
    "        newOut[:,:60,:,:] = out\n",
    "        for j in range(batch_sz):\n",
    "#             print(\"agent_id\",agent_id)\n",
    "#             print(\"track_id\",track_id)\n",
    "            agentIndex = numpy.where(track_id[j]==agent_id[j])\n",
    "            for i in range(30):\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    timeStart = time.time()\n",
    "                    output,hidden = model(inp[j].float().cuda(),1)\n",
    "                    timeTotalModel += time.time() - timeStart\n",
    "                    \n",
    "                initHidden = hidden \n",
    "                hn,cn = initHidden\n",
    "                \n",
    "                \n",
    "                x = inp[j,:60,:,:4]\n",
    "                x = torch.roll(x,-1,dims=1)\n",
    "                inp[j,:60,:19,:4] = x\n",
    "                inp[j,:60,18,:4] = out[j,:60,i,:]\n",
    "                \n",
    "                x = output[agentIndex]\n",
    "                output[:,:] = newOut[j,:,i,:]\n",
    "#                 print(output)\n",
    "                output[agentIndex]= x\n",
    "                \n",
    "                timeStart = time.time()\n",
    "                loss = nn.MSELoss()\n",
    "                loss = loss(output.cuda(),newOut[j,:,i,:].cuda())\n",
    "#                 print(loss)\n",
    "                scaler.scale(loss).backward(retain_graph=True)\n",
    "                scaled_loss += loss.item()\n",
    "                timeTotalLoss = time.time() - timeStart\n",
    "        scaler.step(optimizer)\n",
    "        batch.append(scaled_loss/(30 * batch_sz))\n",
    "        scaler.update()\n",
    "#         print(\"timeEnumeration\",timeEnumeration)\n",
    "#         print(\"timeTotalLoss \",timeTotalLoss)\n",
    "#         print(\"timeTotalModel \",timeTotalModel)\n",
    "        timeTotalLoss = timeTotalModel = 0\n",
    "        if i_batch % math.floor(10) == 0:\n",
    "            print(\"rowsSeen #: \",i_batch * batch_sz,\" avg loss:\",mean(batch))\n",
    "            batch = []\n",
    "\n",
    "# from statistics import mean\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import math\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# agent_id = 0\n",
    "# learning_rate = 1\n",
    "# momentum = 0.1\n",
    "# device = torch.device(\"cuda:0\")\n",
    "# input_dim = 12    # input dimension\n",
    "# hidden_dim = 1  # hidden layer dimension\n",
    "# layer_dim = 10     # number of hidden layers\n",
    "# output_dim = 4   # output dimension\n",
    "\n",
    "# n_epochs = 5\n",
    "# lr=0.01\n",
    "\n",
    "# # Define Loss, Optimizer\n",
    "# #model = RNNModel(input_dim, output_dim, hidden_dim, layer_dim).to(device)\n",
    "# model = RNNModel(input_size=input_dim, output_size=output_dim, hidden_dim=12, n_layers=1)\n",
    "# model = model.to(device)\n",
    "# # model = DataParallel(model,device)\n",
    "# optimizer = optim.Adagrad(model.parameters(), lr=learning_rate,lr_decay=0.00000001)\n",
    "# scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    \n",
    "# model.train()\n",
    "\n",
    "# print(\"test\")\n",
    "# newOut = torch.zeros((batch_sz,100,30,4))\n",
    "# batch = []\n",
    "# for i_epoch in range(n_epochs):\n",
    "#     for i_batch, sample_batch in enumerate(train_loader):\n",
    "#     #     print(\"test\")\n",
    "#         inp, out = sample_batch\n",
    "# #         print(inp[0])\n",
    "#         optimizer.zero_grad()\n",
    "#         initHidden = 1\n",
    "#         scaled_loss = 0\n",
    "#         newOut[:,:60,:,:] = out\n",
    "#         for j in range(batch_sz):\n",
    "#             for i in range(30):\n",
    "\n",
    "#                 with torch.cuda.amp.autocast():\n",
    "#                     output,hidden = model(inp[j].float().cuda(),initHidden)\n",
    "\n",
    "#                 initHidden = hidden\n",
    "#                 hn,cn = initHidden\n",
    "\n",
    "# #                 import random\n",
    "# #                 x = random.uniform(0, 1)\n",
    "# #                 if x < 0.5:\n",
    "# #                     continue\n",
    "#                 loss = nn.MSELoss()\n",
    "#                 loss = loss(output.cuda(),newOut[j,:,i,:].cuda())\n",
    "#                 scaler.scale(loss.to(torch.float16)).backward(retain_graph=True)\n",
    "#                 scaled_loss += loss.item()\n",
    "#         scaler.step(optimizer)\n",
    "#         batch.append(scaled_loss/(30 * batch_sz))\n",
    "#         scaler.update()\n",
    "#         if i_batch % math.floor(100) == 0:\n",
    "#             print(\"rowsSeen #: \",i_batch * batch_sz,\" avg loss (past 100): \",mean(batch))\n",
    "#             batch = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
