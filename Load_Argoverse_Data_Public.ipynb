{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"Change to the data folder\"\"\"\n",
    "new_path = \"../new_train/\"\n",
    "val_path = \"../new_val_in\"\n",
    "\n",
    "# number of sequences in each dataset\n",
    "# train:205942  val:3200 test: 36272 \n",
    "# sequences sampled at 10HZ rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, data_path: str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pkl_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pkl_path = self.pkl_list[idx]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "# intialize a dataset\n",
    "train_dataset  = ArgoverseDataset(data_path=new_path)\n",
    "val_dataset = ArgoverseDataset(data_path=val_path)\n",
    "#print((val_dataset[0]))\n",
    "#print(len(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a loader to enable batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 1\n",
    "\n",
    "def my_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    inp = [numpy.dstack([scene['p_in'], scene['v_in']]) for scene in batch]\n",
    "    out = [numpy.dstack([scene['p_out'], scene['v_out']]) for scene in batch]\n",
    "# print(inp.size)\n",
    "    print(\"gap\")\n",
    "#     print(out.size)\n",
    "    #inp = np.concatenate((inp, out), axis=0)\n",
    "    inp = torch.LongTensor(inp)\n",
    "    print(inp.shape)\n",
    "    print(\"after\")\n",
    "    print(inp)\n",
    "    out = torch.LongTensor(out)\n",
    "    return [inp, out]\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def val_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    inp = [numpy.dstack([scene['p_in'], scene['v_in']]) for scene in batch]\n",
    "    inp = torch.LongTensor(inp)\n",
    "    return inp\n",
    "\n",
    "val_loader = DataLoader(val_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Number of hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # RNN\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True, nonlinearity='relu')\n",
    "        #print(self.rnn)\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        #print(hidden.shape)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "        \"\"\"\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "            \n",
    "        # One time step\n",
    "        print(x)\n",
    "        #print(\"gap\")\n",
    "        #print(h0.shape)\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n",
    "        \"\"\"\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_a_histogram(sample_batch, agent_id, xPos, yPos, xVel, yVel):\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    #agent_sz = inp.size(1)\n",
    "    \n",
    "    for i in range(batch_sz):\n",
    "        #hist_data_xPos = np.zeros((60,19));\n",
    "        #hist_data_yPos = np.zeros((60,19));\n",
    "        #hist_data_xVel = np.zeros((60,19));\n",
    "        hist_data_yVel = np.zeros((60,19));\n",
    "        \n",
    "        for j in range(60):\n",
    "            #hist_data_xPos[j] = (inp[i, j,:,0])\n",
    "            #hist_data_yPos[j] = (inp[i, j,:,1])\n",
    "            #hist_data_xVel[j] = (inp[i, j,:,2])\n",
    "            hist_data_yVel[j] = (inp[i, j,:,3])\n",
    "            \n",
    "        for j in range(len(hist_data_yVel)):\n",
    "            for k in range(len(hist_data_yVel[j])):\n",
    "                #xPos.append(hist_data_xPos[j][k])\n",
    "                #yPos.append(hist_data_yPos[j][k])\n",
    "                #xVel.append(hist_data_xVel[j][k])\n",
    "                yVel.append(hist_data_yVel[j][k])\n",
    "    \n",
    "    \"\"\"\n",
    "    hist_data_xPos = np.zeros((60,19));\n",
    "    hist_data_yPos = np.zeros((60,19));\n",
    "    hist_data_xVel = np.zeros((60,19));\n",
    "    hist_data_yVel = np.zeros((60,19));\n",
    "    \n",
    "    for i in range(60):\n",
    "        hist_data_xPos[i] = (inp[0, i,:,0])\n",
    "        hist_data_yPos[i] = (inp[0, i,:,1])\n",
    "        hist_data_xVel[i] = (inp[0, i,:,2])\n",
    "        hist_data_yVel[i] = (inp[0, i,:,3])\n",
    "    \n",
    "    xPos = np.zeros(60*19)\n",
    "    for i in range(len(hist_data_xPos)):\n",
    "        for j in range(len(hist_data_xPos[i])):\n",
    "            xPos[i*19+j] = hist_data_xPos[i][j]\n",
    "    \n",
    "    #hist_data_xPos = hist_data_xPos.flatten()\n",
    "    hist_data_yVel = hist_data_yPos.flatten()\n",
    "    hist_data_xPos = hist_data_xVel.flatten()\n",
    "    hist_data_yVel = hist_data_yVel.flatten()\n",
    "    #print(xPos)\n",
    "    \n",
    "    n,bins,patches = plt.hist(x=xPos,bins='auto',alpha=0.7,rwidth=0.85)\n",
    "    plt.grid(axis='y',alpha=0.75)\n",
    "    maxfreq = n.max()\n",
    "    plt.ylim(ymax=np.ceil(maxfreq/10) * 10 if maxfreq % 10 else maxfreq + 10)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_batch(sample_batch, agent_id):\n",
    "    \"\"\"visualize the trajectory for a batch of samples with a randon agent\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i, agent_id,:,0], inp[i, agent_id,:,1])\n",
    "        axs[i].scatter(out[i, agent_id,:,0], out[i, agent_id,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the batch of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gap\n",
      "torch.Size([1, 60, 19, 4])\n",
      "after\n",
      "tensor([[[[3277, 1947,    0,    0],\n",
      "          [3277, 1947,    0,    0],\n",
      "          [3277, 1947,    0,    0],\n",
      "          ...,\n",
      "          [3277, 1947,    0,    0],\n",
      "          [3277, 1947,    0,    0],\n",
      "          [3277, 1947,    0,    0]],\n",
      "\n",
      "         [[3277, 1977,    0,    0],\n",
      "          [3277, 1977,    0,    0],\n",
      "          [3277, 1977,    1,    1],\n",
      "          ...,\n",
      "          [3277, 1977,    0,   -1],\n",
      "          [3277, 1977,    0,    0],\n",
      "          [3277, 1977,    0,    0]],\n",
      "\n",
      "         [[3232, 1922,    0,    0],\n",
      "          [3232, 1922,    0,   -1],\n",
      "          [3232, 1923,    0,    2],\n",
      "          ...,\n",
      "          [3232, 1922,    0,    0],\n",
      "          [3232, 1922,    0,    0],\n",
      "          [3232, 1922,    0,    0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[   0,    0,    0,    0],\n",
      "          [   0,    0,    0,    0],\n",
      "          [   0,    0,    0,    0],\n",
      "          ...,\n",
      "          [   0,    0,    0,    0],\n",
      "          [   0,    0,    0,    0],\n",
      "          [   0,    0,    0,    0]],\n",
      "\n",
      "         [[   0,    0,    0,    0],\n",
      "          [   0,    0,    0,    0],\n",
      "          [   0,    0,    0,    0],\n",
      "          ...,\n",
      "          [   0,    0,    0,    0],\n",
      "          [   0,    0,    0,    0],\n",
      "          [   0,    0,    0,    0]],\n",
      "\n",
      "         [[   0,    0,    0,    0],\n",
      "          [   0,    0,    0,    0],\n",
      "          [   0,    0,    0,    0],\n",
      "          ...,\n",
      "          [   0,    0,    0,    0],\n",
      "          [   0,    0,    0,    0],\n",
      "          [   0,    0,    0,    0]]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-4e28cec46930>:11: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  inp = torch.LongTensor(inp)\n",
      "<ipython-input-3-4e28cec46930>:15: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  out = torch.LongTensor(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-3.69117981e+02 -8.43493958e+01  1.02473801e+02 -3.33031219e+02]\n",
      "   [-1.37434525e+02 -1.33703964e+02 -4.72872276e+01 -8.11193924e+01]\n",
      "   [-2.67913570e+01 -6.65094604e+01 -5.07154884e+01 -7.52995834e+01]\n",
      "   ...\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]]\n",
      "\n",
      "  [[-3.70553619e+02 -8.64100723e+01  1.02465767e+02 -3.33263763e+02]\n",
      "   [-1.37782471e+02 -1.33623444e+02 -4.65024719e+01 -8.25036621e+01]\n",
      "   [-2.71529083e+01 -6.74711151e+01 -5.16878090e+01 -7.52501678e+01]\n",
      "   ...\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]]\n",
      "\n",
      "  [[-3.64133301e+02 -8.33060913e+01  1.01062294e+02 -3.28468567e+02]\n",
      "   [-1.35573868e+02 -1.31879623e+02 -4.66353035e+01 -8.00256271e+01]\n",
      "   [-2.64764614e+01 -6.56788406e+01 -5.01021194e+01 -7.42759857e+01]\n",
      "   ...\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]\n",
      "   ...\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]]\n",
      "\n",
      "  [[-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]\n",
      "   ...\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]]\n",
      "\n",
      "  [[-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]\n",
      "   ...\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]\n",
      "   [-2.12396741e-01  1.43416166e-01 -3.69407833e-01  9.62876678e-02]]]]\n",
      "tensor torch.Size([60, 30, 4])\n",
      "out torch.Size([60, 30, 4])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (60).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-5049b7c70635>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m#print(out[0].shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensorOut\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda31\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2260\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2261\u001b[1;33m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0m\u001b[0;32m   2262\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[0;32m   2263\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (60)."
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "agent_id = 0\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "device = \"cpu\"\n",
    "input_dim = 4    # input dimension\n",
    "hidden_dim = 100  # hidden layer dimension\n",
    "layer_dim = 1     # number of hidden layers\n",
    "output_dim = 4   # output dimension\n",
    "\n",
    "n_epochs = 100\n",
    "lr=0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "#model = RNNModel(input_dim, output_dim, hidden_dim, layer_dim).to(device)\n",
    "model = RNNModel(input_size=input_dim, output_size=output_dim, hidden_dim=12, n_layers=1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,momentum=momentum)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "model.train()\n",
    "\n",
    "for i_batch, sample_batch in enumerate(train_loader):\n",
    "    inp, out = sample_batch\n",
    "    #make_a_histogram(sample_batch, agent_id, xPos, yPos, xVel, yVel)\n",
    "    \"\"\"TODO:\n",
    "      Deep learning model\n",
    "      training routine\n",
    "    \"\"\"\n",
    "    inp, out = inp.to(device), out.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outList = np.zeros((1,60,30,4))\n",
    "#     print(outList[0])\n",
    "#     print(inp.shape)\n",
    "    for i in range(30):\n",
    "        output = model(inp[0].float())\n",
    "        output = output.reshape(1,60,19,4)\n",
    "#         print(\"output\",output.shape)\n",
    "        for j in range(60):\n",
    "#             print(output[0][j][18])\n",
    "#             print(outList[0][j][18])\n",
    "            outList[0][j][i] = output[0][j][18].detach().numpy()\n",
    "        for k in range(60):\n",
    "            for j in range(1,19):\n",
    "                inp[0][k][j-1] = inp[0][k][j]\n",
    "            inp[0][k][18] = output[0][k][18]\n",
    "    print(outList)\n",
    "    tensorOut = torch.tensor(outList)\n",
    "    print(\"tensor\",tensorOut[0].shape)\n",
    "    #print(output)\n",
    "    print(\"out\",out[0].shape)\n",
    "    #print(out[0].shape)\n",
    "    \n",
    "    loss = F.nll_loss(tensorOut,out[0])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    show_sample_batch(sample_batch, agent_id)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
