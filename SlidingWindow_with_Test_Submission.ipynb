{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\"\"\"Change to the data folder\"\"\"\n",
    "new_path = \"../new_train/new_train/\"\n",
    "val_path = \"../new_val_in/new_val_in/\"\n",
    "\n",
    "# number of sequences in each dataset\n",
    "# train:205942  val:3200 test: 36272 \n",
    "# sequences sampled at 10HZ rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, data_path: str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pkl_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pkl_path = self.pkl_list[idx]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            \n",
    "#         data['p_in']-= numpy.mean(data['p_in'])\n",
    "#         if self.transform:\n",
    "#             min_max_scaler= preprocessing.MinMaxScaler()\n",
    "#             for i in range(60):\n",
    "#                 data['p_in'][i] = min_max_scaler.fit_transform(data['p_in'][i])\n",
    "#                 data['v_in'][i] = min_max_scaler.fit_transform(data['v_in'][i])\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "# intialize a dataset\n",
    "train_dataset  = ArgoverseDataset(data_path=new_path,transform=True)\n",
    "val_dataset = ArgoverseDataset(data_path=val_path,transform=True)\n",
    "#print((val_dataset[0]))\n",
    "#print(len(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a loader to enable batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_sz = 1\n",
    "\n",
    "def my_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "#     if(len(str(maxVal)) > 3):\n",
    "#         print(len(maxVal))\n",
    "#     print(\"maxStringLength\",len(str(maxVal)))\n",
    "#     print(\"k   \",len(maxVal))\n",
    "    \n",
    "    inp = []\n",
    "    out = []\n",
    "    numbRows = 1000\n",
    "    for scene in batch:\n",
    "#         print(scene['p_in'])\n",
    "        lanes = numpy.zeros((numbRows,19,2))\n",
    "        lane_norm = numpy.zeros((numbRows,19,2))\n",
    "        pIn = vIn = numpy.zeros((numbRows,19,2))\n",
    "#         lane_norm =[0,0]\n",
    "        lengthLane = min(numbRows,len(scene['lane']))\n",
    "        pIn[:len(scene['p_in']),:,:2] = scene['p_in']\n",
    "        vIn[:len(scene['v_in']),:,:2] = scene['v_in']\n",
    "        lanes[:lengthLane,0,:2] = scene['lane'][:lengthLane,:2]\n",
    "        lane_norm[:lengthLane,0,:2] = scene['lane_norm'] [:lengthLane,:2]\n",
    "        inp.append(numpy.dstack([pIn,vIn,lanes,lane_norm]))\n",
    "        out.append(numpy.dstack([scene['p_out'], scene['v_out']]))\n",
    "        \n",
    "    inp = torch.FloatTensor(inp)\n",
    "    out = torch.FloatTensor(out)\n",
    "    return [inp, out]\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz, shuffle = True, collate_fn=my_collate, num_workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def val_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    inp = []\n",
    "    numbRows = 1000\n",
    "    for scene in batch:\n",
    "#         print(scene['p_in'])\n",
    "        lanes = numpy.zeros((numbRows,19,2))\n",
    "        lane_norm = numpy.zeros((numbRows,19,2))\n",
    "        pIn = vIn = numpy.zeros((numbRows,19,2))\n",
    "        lengthLane = min(numbRows,len(scene['lane']))\n",
    "        pIn[:len(scene['p_in']),:,:2] = scene['p_in']\n",
    "        vIn[:len(scene['v_in']),:,:2] = scene['v_in']\n",
    "        lanes[:lengthLane,0,:2] = scene['lane'][:lengthLane,:2]\n",
    "        lane_norm[:lengthLane,0,:2] = scene['lane_norm'] [:lengthLane,:2]\n",
    "        inp.append(numpy.dstack([pIn,vIn,lanes,lane_norm]))\n",
    "        scene_idx = np.array([scene['scene_idx'] for scene in batch])\n",
    "        agent_id = np.array([scene['agent_id'] for scene in batch], dtype=object)\n",
    "        track_id = np.stack([scene['track_id'][:,0] for scene in batch])\n",
    "        \n",
    "    inp = torch.FloatTensor(inp)\n",
    "    return [inp, scene_idx, agent_id, track_id]\n",
    "\n",
    "val_loader = DataLoader(val_dataset,batch_size=batch_sz, shuffle = True, collate_fn=val_collate, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "#         # Number of hidden dimensions\n",
    "#         self.hidden_dim = hidden_dim\n",
    "        \n",
    "#         # Number of hidden layers\n",
    "#         self.n_layers = n_layers\n",
    "        \n",
    "#         # RNN\n",
    "#         self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True, nonlinearity='relu')\n",
    "#         #print(self.rnn)\n",
    "        \n",
    "#         # Readout layer\n",
    "#         self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         batch_size = x.size(0)\n",
    "        \n",
    "#         hidden = self.init_hidden(batch_size)\n",
    "#         #print(hidden.shape)\n",
    "#         out, hidden = self.rnn(x, hidden)\n",
    "#         out = out.contiguous().view(-1, self.hidden_dim)\n",
    "#         out = self.fc(out)\n",
    "        \n",
    "#         return out, hidden\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True,dropout=0.5)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    def forward(self, x,previous):\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        x = x.to(device)\n",
    "        h0 = 0\n",
    "        c0 = 0\n",
    "        if(previous == 1):\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim,device=device).requires_grad_()\n",
    "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim,device=device).requires_grad_()\n",
    "        else:\n",
    "            hn,cn = previous\n",
    "            h0 = hn\n",
    "            c0 = cn\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "#         print(\"forward outshape\",out.shape)\n",
    "        return out,(hn,cn)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim,device=torch.device(\"cuda:0\"))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "Epoch#: 0  batch#: 0  avg loss (past 100):  29587.2453125\n",
      "Epoch#: 0  batch#: 100  avg loss (past 100):  5695.765822682699\n",
      "Epoch#: 0  batch#: 200  avg loss (past 100):  8932.263372131347\n",
      "Epoch#: 0  batch#: 300  avg loss (past 100):  8091.572096872966\n",
      "Epoch#: 0  batch#: 400  avg loss (past 100):  12413.940101476033\n",
      "Epoch#: 0  batch#: 500  avg loss (past 100):  22483.81013597107\n",
      "Epoch#: 0  batch#: 600  avg loss (past 100):  14709.850965820313\n",
      "Epoch#: 0  batch#: 700  avg loss (past 100):  14821.918881103516\n",
      "Epoch#: 0  batch#: 800  avg loss (past 100):  12709.192254923502\n",
      "Epoch#: 0  batch#: 900  avg loss (past 100):  10898.28722302246\n",
      "Epoch#: 0  batch#: 1000  avg loss (past 100):  9389.353409413656\n",
      "Epoch#: 0  batch#: 1100  avg loss (past 100):  8539.028414805094\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "agent_id = 0\n",
    "learning_rate = 50\n",
    "momentum = 0.1\n",
    "device = torch.device(\"cuda:0\")\n",
    "input_dim = 8    # input dimension\n",
    "hidden_dim = 8  # hidden layer dimension\n",
    "layer_dim = 1    # number of hidden layers\n",
    "output_dim = 4   # output dimension\n",
    "\n",
    "n_epochs = 5\n",
    "lr=0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "#model = RNNModel(input_dim, output_dim, hidden_dim, layer_dim).to(device)\n",
    "model = RNNModel(input_size=input_dim, output_size=output_dim, hidden_dim=hidden_dim, n_layers=layer_dim)\n",
    "# model.load_state_dict(torch.load('./sliding3epoch.pth'))\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate,lr_decay=0.00000001)\n",
    "model.eval()\n",
    "\n",
    "    \n",
    "model.train()\n",
    "\n",
    "print(\"test\")\n",
    "newOut = torch.zeros((1000,30,4))\n",
    "batch = []\n",
    "trainLossOverTime = numpy.zeros((205942,5))\n",
    "try:\n",
    "    for i_epoch in range(n_epochs):\n",
    "        for i_batch, sample_batch in enumerate(train_loader):\n",
    "        #     print(\"test\")\n",
    "            inp, out = sample_batch\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            newOut[:60,:,:] = out[0]\n",
    "            initHidden = 1\n",
    "            scaled_loss = 0\n",
    "        #     print(inp)\n",
    "            for i in range(30):\n",
    "\n",
    "                output,hidden = model(inp[0].float().cuda(),initHidden)\n",
    "\n",
    "                initHidden = hidden\n",
    "                hn,cn = initHidden\n",
    "\n",
    "                x = inp[0,:60,:,:4]\n",
    "                x = torch.roll(x,-1,dims=1)\n",
    "                inp[0,:60,:19,:4] = x\n",
    "                inp[0,:60,18,:4] = out[0,:60,i,:]\n",
    "                loss = nn.MSELoss()\n",
    "                loss = loss(output.cuda(),newOut[:,i,:].cuda())\n",
    "                loss.backward(retain_graph=True)\n",
    "                scaled_loss += loss.item()\n",
    "                optimizer.step()\n",
    "            batch.append(scaled_loss/30)\n",
    "            trainLossOverTime[i_batch,i_epoch] = scaled_loss\n",
    "\n",
    "            if i_batch % 100 == 0:\n",
    "                print(\"Epoch#:\", i_epoch, \" batch#:\",i_batch ,\" avg loss (past 100): \",mean(batch))\n",
    "                batch = []\n",
    "                \n",
    "except KeyboardInterrupt:\n",
    "    torch.save(model.state_dict(), './sliding3epoch.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './sliding3epoch.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (lstm): LSTM(8, 8, batch_first=True, dropout=0.5)\n",
       "  (fc): Linear(in_features=8, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "testmodel = RNNModel(input_size=input_dim, output_size=output_dim, hidden_dim=hidden_dim, n_layers=layer_dim)\n",
    "testmodel.load_state_dict(torch.load('sliding3epoch.pth'))\n",
    "testmodel = testModel.to(device)\n",
    "testmodel.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10', 'v11', 'v12', 'v13', 'v14', 'v15', 'v16', 'v17', 'v18', 'v19', 'v20', 'v21', 'v22', 'v23', 'v24', 'v25', 'v26', 'v27', 'v28', 'v29', 'v30', 'v31', 'v32', 'v33', 'v34', 'v35', 'v36', 'v37', 'v38', 'v39', 'v40', 'v41', 'v42', 'v43', 'v44', 'v45', 'v46', 'v47', 'v48', 'v49', 'v50', 'v51', 'v52', 'v53', 'v54', 'v55', 'v56', 'v57', 'v58', 'v59', 'v60']\n",
      "batch #:  0\n",
      "batch #:  10\n",
      "batch #:  20\n",
      "batch #:  30\n",
      "batch #:  40\n",
      "batch #:  50\n",
      "batch #:  60\n",
      "batch #:  70\n",
      "batch #:  80\n",
      "batch #:  90\n",
      "batch #:  100\n",
      "batch #:  110\n",
      "batch #:  120\n",
      "batch #:  130\n",
      "batch #:  140\n",
      "batch #:  150\n",
      "batch #:  160\n",
      "batch #:  170\n",
      "batch #:  180\n",
      "batch #:  190\n",
      "batch #:  200\n",
      "batch #:  210\n",
      "batch #:  220\n",
      "batch #:  230\n",
      "batch #:  240\n",
      "batch #:  250\n",
      "batch #:  260\n",
      "batch #:  270\n",
      "batch #:  280\n",
      "batch #:  290\n",
      "batch #:  300\n",
      "batch #:  310\n",
      "batch #:  320\n",
      "batch #:  330\n",
      "batch #:  340\n",
      "batch #:  350\n",
      "batch #:  360\n",
      "batch #:  370\n",
      "batch #:  380\n",
      "batch #:  390\n",
      "batch #:  400\n",
      "batch #:  410\n",
      "batch #:  420\n",
      "batch #:  430\n",
      "batch #:  440\n",
      "batch #:  450\n",
      "batch #:  460\n",
      "batch #:  470\n",
      "batch #:  480\n",
      "batch #:  490\n",
      "batch #:  500\n",
      "batch #:  510\n",
      "batch #:  520\n",
      "batch #:  530\n",
      "batch #:  540\n",
      "batch #:  550\n",
      "batch #:  560\n",
      "batch #:  570\n",
      "batch #:  580\n",
      "batch #:  590\n",
      "batch #:  600\n",
      "batch #:  610\n",
      "batch #:  620\n",
      "batch #:  630\n",
      "batch #:  640\n",
      "batch #:  650\n",
      "batch #:  660\n",
      "batch #:  670\n",
      "batch #:  680\n",
      "batch #:  690\n",
      "batch #:  700\n",
      "batch #:  710\n",
      "batch #:  720\n",
      "batch #:  730\n",
      "batch #:  740\n",
      "batch #:  750\n",
      "batch #:  760\n",
      "batch #:  770\n",
      "batch #:  780\n",
      "batch #:  790\n",
      "batch #:  800\n",
      "batch #:  810\n",
      "batch #:  820\n",
      "batch #:  830\n",
      "batch #:  840\n",
      "batch #:  850\n",
      "batch #:  860\n",
      "batch #:  870\n",
      "batch #:  880\n",
      "batch #:  890\n",
      "batch #:  900\n",
      "batch #:  910\n",
      "batch #:  920\n",
      "batch #:  930\n",
      "batch #:  940\n",
      "batch #:  950\n",
      "batch #:  960\n",
      "batch #:  970\n",
      "batch #:  980\n",
      "batch #:  990\n",
      "batch #:  1000\n",
      "batch #:  1010\n",
      "batch #:  1020\n",
      "batch #:  1030\n",
      "batch #:  1040\n",
      "batch #:  1050\n",
      "batch #:  1060\n",
      "batch #:  1070\n",
      "batch #:  1080\n",
      "batch #:  1090\n",
      "batch #:  1100\n",
      "batch #:  1110\n",
      "batch #:  1120\n",
      "batch #:  1130\n",
      "batch #:  1140\n",
      "batch #:  1150\n",
      "batch #:  1160\n",
      "batch #:  1170\n",
      "batch #:  1180\n",
      "batch #:  1190\n",
      "batch #:  1200\n",
      "batch #:  1210\n",
      "batch #:  1220\n",
      "batch #:  1230\n",
      "batch #:  1240\n",
      "batch #:  1250\n",
      "batch #:  1260\n",
      "batch #:  1270\n",
      "batch #:  1280\n",
      "batch #:  1290\n",
      "batch #:  1300\n",
      "batch #:  1310\n",
      "batch #:  1320\n",
      "batch #:  1330\n",
      "batch #:  1340\n",
      "batch #:  1350\n",
      "batch #:  1360\n",
      "batch #:  1370\n",
      "batch #:  1380\n",
      "batch #:  1390\n",
      "batch #:  1400\n",
      "batch #:  1410\n",
      "batch #:  1420\n",
      "batch #:  1430\n",
      "batch #:  1440\n",
      "batch #:  1450\n",
      "batch #:  1460\n",
      "batch #:  1470\n",
      "batch #:  1480\n",
      "batch #:  1490\n",
      "batch #:  1500\n",
      "batch #:  1510\n",
      "batch #:  1520\n",
      "batch #:  1530\n",
      "batch #:  1540\n",
      "batch #:  1550\n",
      "batch #:  1560\n",
      "batch #:  1570\n",
      "batch #:  1580\n",
      "batch #:  1590\n",
      "batch #:  1600\n",
      "batch #:  1610\n",
      "batch #:  1620\n",
      "batch #:  1630\n",
      "batch #:  1640\n",
      "batch #:  1650\n",
      "batch #:  1660\n",
      "batch #:  1670\n",
      "batch #:  1680\n",
      "batch #:  1690\n",
      "batch #:  1700\n",
      "batch #:  1710\n",
      "batch #:  1720\n",
      "batch #:  1730\n",
      "batch #:  1740\n",
      "batch #:  1750\n",
      "batch #:  1760\n",
      "batch #:  1770\n",
      "batch #:  1780\n",
      "batch #:  1790\n",
      "batch #:  1800\n",
      "batch #:  1810\n",
      "batch #:  1820\n",
      "batch #:  1830\n",
      "batch #:  1840\n",
      "batch #:  1850\n",
      "batch #:  1860\n",
      "batch #:  1870\n",
      "batch #:  1880\n",
      "batch #:  1890\n",
      "batch #:  1900\n",
      "batch #:  1910\n",
      "batch #:  1920\n",
      "batch #:  1930\n",
      "batch #:  1940\n",
      "batch #:  1950\n",
      "batch #:  1960\n",
      "batch #:  1970\n",
      "batch #:  1980\n",
      "batch #:  1990\n",
      "batch #:  2000\n",
      "batch #:  2010\n",
      "batch #:  2020\n",
      "batch #:  2030\n",
      "batch #:  2040\n",
      "batch #:  2050\n",
      "batch #:  2060\n",
      "batch #:  2070\n",
      "batch #:  2080\n",
      "batch #:  2090\n",
      "batch #:  2100\n",
      "batch #:  2110\n",
      "batch #:  2120\n",
      "batch #:  2130\n",
      "batch #:  2140\n",
      "batch #:  2150\n",
      "batch #:  2160\n",
      "batch #:  2170\n",
      "batch #:  2180\n",
      "batch #:  2190\n",
      "batch #:  2200\n",
      "batch #:  2210\n",
      "batch #:  2220\n",
      "batch #:  2230\n",
      "batch #:  2240\n",
      "batch #:  2250\n",
      "batch #:  2260\n",
      "batch #:  2270\n",
      "batch #:  2280\n",
      "batch #:  2290\n",
      "batch #:  2300\n",
      "batch #:  2310\n",
      "batch #:  2320\n",
      "batch #:  2330\n",
      "batch #:  2340\n",
      "batch #:  2350\n",
      "batch #:  2360\n",
      "batch #:  2370\n",
      "batch #:  2380\n",
      "batch #:  2390\n",
      "batch #:  2400\n",
      "batch #:  2410\n",
      "batch #:  2420\n",
      "batch #:  2430\n",
      "batch #:  2440\n",
      "batch #:  2450\n",
      "batch #:  2460\n",
      "batch #:  2470\n",
      "batch #:  2480\n",
      "batch #:  2490\n",
      "batch #:  2500\n",
      "batch #:  2510\n",
      "batch #:  2520\n",
      "batch #:  2530\n",
      "batch #:  2540\n",
      "batch #:  2550\n",
      "batch #:  2560\n",
      "batch #:  2570\n",
      "batch #:  2580\n",
      "batch #:  2590\n",
      "batch #:  2600\n",
      "batch #:  2610\n",
      "batch #:  2620\n",
      "batch #:  2630\n",
      "batch #:  2640\n",
      "batch #:  2650\n",
      "batch #:  2660\n",
      "batch #:  2670\n",
      "batch #:  2680\n",
      "batch #:  2690\n",
      "batch #:  2700\n",
      "batch #:  2710\n",
      "batch #:  2720\n",
      "batch #:  2730\n",
      "batch #:  2740\n",
      "batch #:  2750\n",
      "batch #:  2760\n",
      "batch #:  2770\n",
      "batch #:  2780\n",
      "batch #:  2790\n",
      "batch #:  2800\n",
      "batch #:  2810\n",
      "batch #:  2820\n",
      "batch #:  2830\n",
      "batch #:  2840\n",
      "batch #:  2850\n",
      "batch #:  2860\n",
      "batch #:  2870\n",
      "batch #:  2880\n",
      "batch #:  2890\n",
      "batch #:  2900\n",
      "batch #:  2910\n",
      "batch #:  2920\n",
      "batch #:  2930\n",
      "batch #:  2940\n",
      "batch #:  2950\n",
      "batch #:  2960\n",
      "batch #:  2970\n",
      "batch #:  2980\n",
      "batch #:  2990\n",
      "batch #:  3000\n",
      "batch #:  3010\n",
      "batch #:  3020\n",
      "batch #:  3030\n",
      "batch #:  3040\n",
      "batch #:  3050\n",
      "batch #:  3060\n",
      "batch #:  3070\n",
      "batch #:  3080\n",
      "batch #:  3090\n",
      "batch #:  3100\n",
      "batch #:  3110\n",
      "batch #:  3120\n",
      "batch #:  3130\n",
      "batch #:  3140\n",
      "batch #:  3150\n",
      "batch #:  3160\n",
      "batch #:  3170\n",
      "batch #:  3180\n",
      "batch #:  3190\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "save_file = \"submissionsliding3.csv\"\n",
    "\n",
    "header = [\"ID\"]\n",
    "header += [\"v\"+str(x) for x in range(1, 61)]\n",
    "print(header)\n",
    "\n",
    "with open(save_file, 'w') as f:\n",
    "    f.write(\",\".join(header)+\"\\n\")\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# input_dim = 8    # input dimension\n",
    "# hidden_dim = 8  # hidden layer dimension\n",
    "# layer_dim = 1     # number of hidden layers\n",
    "# output_dim = 4   # output dimension\n",
    "\n",
    "# testmodel = RNNModel(input_size=input_dim, output_size=output_dim, hidden_dim=hidden_dim, n_layers=layer_dim)\n",
    "# testmodel.load_state_dict(torch.load('sliding3epoch.pth'))\n",
    "# testmodel.to(device)\n",
    "# testmodel.eval()\n",
    "\n",
    "\n",
    "for i_batch, sample_batch in enumerate(val_loader):\n",
    "    if i_batch % 10 == 0:\n",
    "        print(\"batch #: \", i_batch)\n",
    "    inp, scene_idx, agent_ids, track_ids = sample_batch\n",
    "\n",
    "    bool_loc = np.stack([np.squeeze(track_id) == np.repeat(ag_id,60) for ag_id, track_id in zip(agent_ids, track_ids)])\n",
    "\n",
    "\n",
    "    initHidden = 1\n",
    "    full_out = []\n",
    "    for i in range(30):\n",
    "\n",
    "        output,hidden = testmodel(inp[0].float().to(device),initHidden)\n",
    "        initHidden = hidden\n",
    "        hn,cn = initHidden\n",
    "        full_out.append(output)\n",
    "        x = inp[0,:60,:,:4]\n",
    "\n",
    "        x = torch.roll(x,-1,dims=1)\n",
    "        inp[0,:60, :19, :4] = x\n",
    "        inp[0,:60,18,:4] = output[:60,:]\n",
    "        inp = inp.to(device)\n",
    "\n",
    "    out = torch.stack(full_out)\n",
    "    \n",
    "    \n",
    "    out = out.transpose(0,1)\n",
    "    out = out[:60, :, :2]\n",
    "    out = out.reshape(60, -1)\n",
    "    out = out[bool_loc]\n",
    "    with open(save_file, \"a\") as f:\n",
    "        for row, scene_num in zip(out.detach().cpu().numpy().astype(int).astype(str), scene_idx):\n",
    "            f.write(str(scene_num)+\",\"+\",\".join(row)+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_a_histogram(sample_batch, agent_id, xPos, yPos, xVel, yVel):\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    #agent_sz = inp.size(1)\n",
    "    \n",
    "    for i in range(batch_sz):\n",
    "        #hist_data_xPos = np.zeros((60,19));\n",
    "        #hist_data_yPos = np.zeros((60,19));\n",
    "        #hist_data_xVel = np.zeros((60,19));\n",
    "        hist_data_yVel = np.zeros((60,19));\n",
    "        \n",
    "        for j in range(60):\n",
    "            #hist_data_xPos[j] = (inp[i, j,:,0])\n",
    "            #hist_data_yPos[j] = (inp[i, j,:,1])\n",
    "            #hist_data_xVel[j] = (inp[i, j,:,2])\n",
    "            hist_data_yVel[j] = (inp[i, j,:,3])\n",
    "            \n",
    "        for j in range(len(hist_data_yVel)):\n",
    "            for k in range(len(hist_data_yVel[j])):\n",
    "                #xPos.append(hist_data_xPos[j][k])\n",
    "                #yPos.append(hist_data_yPos[j][k])\n",
    "                #xVel.append(hist_data_xVel[j][k])\n",
    "                yVel.append(hist_data_yVel[j][k])\n",
    "    \n",
    "    \"\"\"\n",
    "    hist_data_xPos = np.zeros((60,19));\n",
    "    hist_data_yPos = np.zeros((60,19));\n",
    "    hist_data_xVel = np.zeros((60,19));\n",
    "    hist_data_yVel = np.zeros((60,19));\n",
    "    \n",
    "    for i in range(60):\n",
    "        hist_data_xPos[i] = (inp[0, i,:,0])\n",
    "        hist_data_yPos[i] = (inp[0, i,:,1])\n",
    "        hist_data_xVel[i] = (inp[0, i,:,2])\n",
    "        hist_data_yVel[i] = (inp[0, i,:,3])\n",
    "    \n",
    "    xPos = np.zeros(60*19)\n",
    "    for i in range(len(hist_data_xPos)):\n",
    "        for j in range(len(hist_data_xPos[i])):\n",
    "            xPos[i*19+j] = hist_data_xPos[i][j]\n",
    "    \n",
    "    #hist_data_xPos = hist_data_xPos.flatten()\n",
    "    hist_data_yVel = hist_data_yPos.flatten()\n",
    "    hist_data_xPos = hist_data_xVel.flatten()\n",
    "    hist_data_yVel = hist_data_yVel.flatten()\n",
    "    #print(xPos)\n",
    "    \n",
    "    n,bins,patches = plt.hist(x=xPos,bins='auto',alpha=0.7,rwidth=0.85)\n",
    "    plt.grid(axis='y',alpha=0.75)\n",
    "    maxfreq = n.max()\n",
    "    plt.ylim(ymax=np.ceil(maxfreq/10) * 10 if maxfreq % 10 else maxfreq + 10)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_sample_batch(sample_batch, agent_id):\n",
    "    \"\"\"visualize the trajectory for a batch of samples with a randon agent\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i, agent_id,:,0], inp[i, agent_id,:,1])\n",
    "        axs[i].scatter(out[i, agent_id,:,0], out[i, agent_id,:,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
