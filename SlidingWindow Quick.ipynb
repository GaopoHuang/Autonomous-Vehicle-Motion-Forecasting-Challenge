{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\"\"\"Change to the data folder\"\"\"\n",
    "new_path = \"../new_train/\"\n",
    "val_path = \"../new_val_in/\"\n",
    "\n",
    "# number of sequences in each dataset\n",
    "# train:205942  val:3200 test: 36272 \n",
    "# sequences sampled at 10HZ rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, data_path: str, training=True):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.training = training\n",
    "\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pkl_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pkl_path = self.pkl_list[idx]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "#         varMin =[-5.37872696e+00,  0.00000000e+00, -6.39434204e+01, -8.02092514e+01,\n",
    "#   0.00000000e+00,  0.00000000e+00,  1.00000000e+06,  1.00000000e+06]\n",
    "#         varMax = [ 4.72452881e+03,  4.08314209e+03,  7.73842545e+01,  0.00000000e+00,\n",
    "#   4.73215820e+03,  4.07677856e+03, -1.00000000e+06, -1.00000000e+06]\n",
    "        columns = ['p_in','p_in','v_in','v_in','lane','lane','lane_norm','lane_norm']\n",
    "        outColumns = ['p_out','p_out','v_out','v_out']\n",
    "    #FOR 1000 rows, no shuffle\n",
    "#     varMin=[ 0.00000000e+00,  0.00000000e+00, -3.83204346e+01, -4.79440918e+01,\n",
    "#   0.00000000e+00,  0.00000000e+00,  1.00000000e+06,  1.00000000e+06]\n",
    "#         varMax=[ 4.70824121e+03,  4.04640869e+03,  7.00706635e+01,  0.00000000e+00,\n",
    "#   4.71727344e+03,  4.07275757e+03, -1.00000000e+06, -1.00000000e+06]\n",
    "#   FOR ALL ROWS\n",
    "        varMaxOutput= [4773.,   4097.7,   193.19,  194.33]\n",
    "        varMinOutput=[ -53.912,    0.,    -210.04,  -187.71 ]\n",
    "        varMaxInput=[4748.2,   4096.1,    252.32,   183.53,  4791.6,   4121.4,     18.801,   16.702]\n",
    "        varMinInput=[ -46.958,    0.,    -222.63,  -179.87,   -75.963,    0.,     -18.564,  -16.691]\n",
    "#         #Changed to 4 because we don't need lane and lane norm\n",
    "        for i in range(8):\n",
    "            j = i % 2\n",
    "            data[columns[i]][j] = (data[columns[i]][j] - varMinInput[i]) / (varMaxInput[i] - varMinInput[i])\n",
    "            if i < 4 and self.training:\n",
    "                data[outColumns[i]][j] = (data[outColumns[i]][j] - varMinOutput[i]) / (varMaxOutput[i] - varMinOutput[i]) \n",
    "#         data['p_out'][0] = (data['p_out'][0] - -5.37872696e+00) / (4.72452881e+03 - -5.37872696e+00)\n",
    "        return data\n",
    "\n",
    "\n",
    "train_dataset  = ArgoverseDataset(data_path=new_path)\n",
    "val_dataset = ArgoverseDataset(data_path=val_path,training=False)\n",
    "#print((val_dataset[0]))\n",
    "#print(len(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a loader to enable batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 200\n",
    "\n",
    "def my_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    inp = []\n",
    "    out = []\n",
    "    laneInfo = []\n",
    "    city = []\n",
    "#     print(\"pIn\",batch[0]['p_in'])\n",
    "    numbRows = 60\n",
    "    for scene in batch:\n",
    "#         print(\"scenePin\",scene['p_in'])\n",
    "        cityyy = numpy.zeros((60,1,4))\n",
    "        #print(cityyy)\n",
    "        if scene['city'] == 'PIT':\n",
    "            cityyy[:,:,:] = 1\n",
    "            #print(\"mummamia\",cityyy)\n",
    "        city.append(numpy.dstack([cityyy]))\n",
    "        lanes = numpy.zeros((numbRows * 19,2))\n",
    "        lane_norm = numpy.zeros((numbRows * 19,2))\n",
    "#         pIn = numpy.zeros((numbRows,19,2))\n",
    "#         vIn = numpy.zeros((numbRows,19,2))\n",
    "        lengthLane = min(numbRows * 19,len(scene['lane']))\n",
    "#         pIn[:len(scene['p_in']),:,:2] = scene['p_in']\n",
    "#         vIn[:len(scene['v_in']),:,:2] = scene['v_in']\n",
    "        \n",
    "#         x = lanes.reshape()\n",
    "        lanes[:lengthLane,:2] = scene['lane'][:lengthLane,:2]\n",
    "        lane_norm[:lengthLane,:2] = scene['lane_norm'] [:lengthLane,:2]\n",
    "        laneInfo.append(numpy.dstack([lanes.reshape(60,19,2),lane_norm.reshape(60,19,2)]))\n",
    "        inp.append(numpy.dstack([scene['p_in'],scene['v_in']]))\n",
    "        out.append(numpy.dstack([scene['p_out'], scene['v_out']]))\n",
    "#     print(\"from mycollate\",scene['p_in']) \n",
    "#     print(\"p_in\",inp[0][0][0])\n",
    "    laneInfo = torch.FloatTensor(laneInfo)\n",
    "    inp = torch.FloatTensor(inp)\n",
    "    out = torch.FloatTensor(out)\n",
    "    city = torch.FloatTensor(city)\n",
    "    return [city,laneInfo,inp, out]\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz, shuffle = True, collate_fn=my_collate, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def val_collate(batch):\n",
    "    agentIds = []\n",
    "    trackIds = []\n",
    "    sceneIdxs = []\n",
    "    laneInfo = []\n",
    "    inp = []\n",
    "    for scene in batch:\n",
    "        \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "        agentIds.append(scene['agent_id'])\n",
    "        trackIds.append(scene['track_id'])\n",
    "        sceneIdxs.append(scene['scene_idx'])\n",
    "        lanes = numpy.zeros((numbRows * 19,2))\n",
    "        lane_norm = numpy.zeros((numbRows * 19,2))\n",
    "        lengthLane = min(numbRows * 19,len(scene['lane']))\n",
    "        lanes[:lengthLane,:2] = scene['lane'][:lengthLane,:2]\n",
    "        lane_norm[:lengthLane,:2] = scene['lane_norm'] [:lengthLane,:2]\n",
    "        laneInfo.append(numpy.dstack([lanes.reshape(60,19,2),lane_norm.reshape(60,19,2)]))\n",
    "        inp.append(numpy.dstack([scene['p_in'], scene['v_in']]))\n",
    "    inp = torch.FloatTensor(inp)\n",
    "    laneInfo = torch.FloatTensor(laneInfo)\n",
    "    return [inp,sceneIdxs,agentIds,trackIds,laneInfo]\n",
    "\n",
    "val_loader = DataLoader(val_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, self.hidden_dim, self.num_layers, batch_first=True,dropout=0.5)\n",
    "        self.fc = nn.Conv1d(self.hidden_dim, 240,1)\n",
    "#         self.bn1 = nn.BatchNorm1d(num_features=4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x,_ = self.lstm(x)\n",
    "#         print(\"shapeee\",x.shape)\n",
    "        x = x.transpose(1,2)\n",
    "#         print(x.shape)\n",
    "        x = self.fc(x)\n",
    "#         print(x.shape)\n",
    "        x = x.transpose(1,2)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def forwardTesting(self,x):\n",
    "        res =[]\n",
    "        h = torch.zeros((self.num_layers,len(x),self.hidden_dim)).cuda()\n",
    "        c = torch.zeros((self.num_layers,len(x),self.hidden_dim)).cuda()\n",
    "        for steps in range(num_steps):\n",
    "            x,(h,c) = self.lstm(x,(h,c))\n",
    "            x = x[:,-1:]\n",
    "            x = x.tranpose(1,2)\n",
    "            x = self.fc(x)\n",
    "            x = x.transpose(1,2)\n",
    "            res.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_a_histogram(sample_batch, agent_id, xPos, yPos, xVel, yVel):\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    #agent_sz = inp.size(1)\n",
    "    \n",
    "    for i in range(batch_sz):\n",
    "        #hist_data_xPos = np.zeros((60,19));\n",
    "        #hist_data_yPos = np.zeros((60,19));\n",
    "        #hist_data_xVel = np.zeros((60,19));\n",
    "        hist_data_yVel = np.zeros((60,19));\n",
    "        \n",
    "        for j in range(60):\n",
    "            #hist_data_xPos[j] = (inp[i, j,:,0])\n",
    "            #hist_data_yPos[j] = (inp[i, j,:,1])\n",
    "            #hist_data_xVel[j] = (inp[i, j,:,2])\n",
    "            hist_data_yVel[j] = (inp[i, j,:,3])\n",
    "            \n",
    "        for j in range(len(hist_data_yVel)):\n",
    "            for k in range(len(hist_data_yVel[j])):\n",
    "                #xPos.append(hist_data_xPos[j][k])\n",
    "                #yPos.append(hist_data_yPos[j][k])\n",
    "                #xVel.append(hist_data_xVel[j][k])\n",
    "                yVel.append(hist_data_yVel[j][k])\n",
    "    \n",
    "    \"\"\"\n",
    "    hist_data_xPos = np.zeros((60,19));\n",
    "    hist_data_yPos = np.zeros((60,19));\n",
    "    hist_data_xVel = np.zeros((60,19));\n",
    "    hist_data_yVel = np.zeros((60,19));\n",
    "    \n",
    "    for i in range(60):\n",
    "        hist_data_xPos[i] = (inp[0, i,:,0])\n",
    "        hist_data_yPos[i] = (inp[0, i,:,1])\n",
    "        hist_data_xVel[i] = (inp[0, i,:,2])\n",
    "        hist_data_yVel[i] = (inp[0, i,:,3])\n",
    "    \n",
    "    xPos = np.zeros(60*19)\n",
    "    for i in range(len(hist_data_xPos)):\n",
    "        for j in range(len(hist_data_xPos[i])):\n",
    "            xPos[i*19+j] = hist_data_xPos[i][j]\n",
    "    \n",
    "    #hist_data_xPos = hist_data_xPos.flatten()\n",
    "    hist_data_yVel = hist_data_yPos.flatten()\n",
    "    hist_data_xPos = hist_data_xVel.flatten()\n",
    "    hist_data_yVel = hist_data_yVel.flatten()\n",
    "    #print(xPos)\n",
    "    \n",
    "    n,bins,patches = plt.hist(x=xPos,bins='auto',alpha=0.7,rwidth=0.85)\n",
    "    plt.grid(axis='y',alpha=0.75)\n",
    "    maxfreq = n.max()\n",
    "    plt.ylim(ymax=np.ceil(maxfreq/10) * 10 if maxfreq % 10 else maxfreq + 10)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_batch(sample_batch, agent_id):\n",
    "    \"\"\"visualize the trajectory for a batch of samples with a randon agent\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i, agent_id,:,0], inp[i, agent_id,:,1])\n",
    "        axs[i].scatter(out[i, agent_id,:,0], out[i, agent_id,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the batch of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "test\n",
      "batch #:  0  avg loss per scene(past 100):  342.82025146484375 342.8202819824219\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "agent_id = 0\n",
    "learning_rate = 1e-3\n",
    "momentum = 0.01\n",
    "device = torch.device(\"cuda:0\")\n",
    "input_dim = 4 * 60    # input dimension\n",
    "hidden_dim = 3000  # hidden layer dimension should be greater when batch_sz small\n",
    "layer_dim = 3    # number of hidden layers\n",
    "output_dim = 4   # output dimension\n",
    "\n",
    "n_epochs = 30\n",
    "# varMin =[-5.37872696e+00,  0.00000000e+00, -6.39434204e+01, -8.02092514e+01,\n",
    "#   0.00000000e+00,  0.00000000e+00,  1.00000000e+06,  1.00000000e+06]\n",
    "# varMax = [ 4.72452881e+03,  4.08314209e+03,  7.73842545e+01,  0.00000000e+00,\n",
    "#   4.73215820e+03,  4.07677856e+03, -1.00000000e+06, -1.00000000e+06]\n",
    "# columns = ['p_in','p_in','v_in','v_in','lane','lane','lane_norm','lane_norm']\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "#model = RNNModel(input_dim, output_dim, hidden_dim, layer_dim).to(device)\n",
    "model = RNNModel(input_size=input_dim, output_size=output_dim, hidden_dim=hidden_dim, n_layers=layer_dim)\n",
    "model.load_state_dict(torch.load('./sliding3epoch.pth'))\n",
    "model = model.to(device)\n",
    "# optimizer = optim.Adagrad(model.parameters(), lr=learning_rate,weight_decay=0.00000001)\n",
    "# optimizer = optim.Adadelta(model.parameters())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay=1e-5)\n",
    "# optimizer = optim.RMSprop(model.parameters(),lr=learning_rate,momentum=momentum)\n",
    "\n",
    "#find mean\n",
    "var = numpy.zeros((1,8))\n",
    "seen = 0\n",
    "print('start')\n",
    "\n",
    "    \n",
    "model.train()\n",
    "\n",
    "print(\"test\")\n",
    "# newOut = torch.zeros((batch_sz,100,30,4))\n",
    "batch = []\n",
    "inpArray = []\n",
    "loss_ema = -1\n",
    "try:\n",
    "    for i_epoch in range(n_epochs):\n",
    "        for i_batch, sample_batch in enumerate(train_loader):\n",
    "        #     print(\"test\")\n",
    "            city,laneInfo,inp, out = sample_batch\n",
    "            laneInfo = laneInfo.cuda()\n",
    "            inp = inp.cuda()\n",
    "            out = out.cuda()\n",
    "            city = city.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            mixed = torch.cat([city,laneInfo,inp,out],2).transpose(1,2).reshape(-1,69,4 * 60)\n",
    "#             print(mixed)\n",
    "            \n",
    "            y_pred = model(mixed[:,:-1])[:,-30:]\n",
    "            y_pred = y_pred.reshape((-1,30,60,4)).transpose(1,2)\n",
    "#             print(y_pred.shape)\n",
    "            \n",
    "#             print(out.shape)\n",
    "            #loss = nn.MSELoss()\n",
    "            #loss = loss(y_pred,out)\n",
    "#             print(y_pred)\n",
    "            loss = (torch.mean((y_pred-out)**2))**0.5\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if loss_ema < 0:\n",
    "                loss_ema = loss\n",
    "            loss_ema= loss_ema*0.90 +loss*0.1\n",
    "\n",
    "            if i_batch % 10 == 0:\n",
    "                print(\"batch #: \",i_batch * batch_sz ,\" avg loss per scene(past 100): \",loss_ema.item(),loss.item())\n",
    "                batch = []\n",
    "                \n",
    "except KeyboardInterrupt:\n",
    "    print(\"savedModel\")\n",
    "    torch.save(model.state_dict(), './sliding3epoch.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './sliding3epoch.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "save_file = \"submissionsliding.csv\"\n",
    "\n",
    "\n",
    "header = [\"ID\"]\n",
    "header += [\"v\"+str(x) for x in range(1, 61)]\n",
    "print(header)\n",
    "\n",
    "with open(save_file, 'w') as f:\n",
    "    f.write(\",\".join(header)+\"\\n\")\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "testmodel = RNNModel(input_size=input_dim, output_size=output_dim, hidden_dim=hidden_dim, n_layers=layer_dim)\n",
    "testmodel.load_state_dict(torch.load('sliding3epoch.pth'))\n",
    "testmodel.to(device)\n",
    "testmodel.eval()\n",
    "\n",
    "full_out = []\n",
    "print(\"test\")\n",
    "\n",
    "print(bool_loc)\n",
    "for i_batch, sample_batch in enumerate(val_loader):\n",
    "    if i_batch % 10 == 0:\n",
    "        print(\"batch #: \", i_batch)\n",
    "#     print(len(sample_batch[1]))\n",
    "#     print(sample_batch[0].shape)\n",
    "    inp, scene_idx, agent_ids, track_ids,laneInfo = sample_batch\n",
    "#     print(agent_id, track_id)\n",
    "    bool_loc = np.stack([np.squeeze(track_id) == np.repeat(ag_id,60) for ag_id, track_id in zip(agent_ids, track_ids)])\n",
    "    mixed = torch.cat([laneInfo,inp],2).transpose(1,2).reshape(-1,38,4 * 60)\n",
    "    y_pred = model.forwardTesting(mixed.reshape(len(mixed),-1)).reshape((-1,60,30,4))\n",
    "    \n",
    "    out = y_pred[bool_loc]\n",
    "#     print(type(out))\n",
    "#     print(\"out shape after: \", out.shape)\n",
    "#     full_out.append(out)\n",
    "#     print(type(scene_idx))\n",
    "#     print(\",\".join(out.detach().cpu().numpy().astype(int).astype(str)[0]))\n",
    "#     fin_out = np.concatenate((scene_idx.reshape(-1,1), out.detach().cpu().numpy().astype(int)), axis=1)\n",
    "    with open(save_file, \"a\") as f:\n",
    "        for row, scene_num in zip(out.detach().cpu().numpy().astype(int).astype(str), scene_idx):\n",
    "            f.write(str(scene_num)+\",\"+\",\".join(row)+\"\\n\")\n",
    "#         np.savetxt(f, fin_out, fmt=\"%cd\",delimiter=\",\", newline=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
