{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\"\"\"Change to the data folder\"\"\"\n",
    "new_path = \"../new_train/\"\n",
    "val_path = \"../new_val_in/\"\n",
    "\n",
    "# number of sequences in each dataset\n",
    "# train:205942  val:3200 test: 36272 \n",
    "# sequences sampled at 10HZ rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, data_path: str, transform=None,training=True):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.training = training\n",
    "\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pkl_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pkl_path = self.pkl_list[idx]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "#         varMin =[-5.37872696e+00,  0.00000000e+00, -6.39434204e+01, -8.02092514e+01,\n",
    "#   0.00000000e+00,  0.00000000e+00,  1.00000000e+06,  1.00000000e+06]\n",
    "#         varMax = [ 4.72452881e+03,  4.08314209e+03,  7.73842545e+01,  0.00000000e+00,\n",
    "#   4.73215820e+03,  4.07677856e+03, -1.00000000e+06, -1.00000000e+06]\n",
    "        columns = ['p_in','p_in','v_in','v_in','lane','lane','lane_norm','lane_norm']\n",
    "        outColumns = ['p_out','p_out','v_out','v_out']\n",
    "    #FOR 1000 rows, no shuffle\n",
    "#     varMin=[ 0.00000000e+00,  0.00000000e+00, -3.83204346e+01, -4.79440918e+01,\n",
    "#   0.00000000e+00,  0.00000000e+00,  1.00000000e+06,  1.00000000e+06]\n",
    "#         varMax=[ 4.70824121e+03,  4.04640869e+03,  7.00706635e+01,  0.00000000e+00,\n",
    "#   4.71727344e+03,  4.07275757e+03, -1.00000000e+06, -1.00000000e+06]\n",
    "#   FOR ALL ROWS\n",
    "#         varMaxOutput= [4773.,   4097.7,   193.19,  194.33]\n",
    "#         varMinOutput=[ -53.912,    0.,    -210.04,  -187.71 ]\n",
    "#         varMaxInput=[4748.2,   4096.1,    252.32,   183.53,  4791.6,   4121.4,     18.801,   16.702]\n",
    "#         varMinInput=[ -46.958,    0.,    -222.63,  -179.87,   -75.963,    0.,     -18.564,  -16.691]\n",
    "#         #Changed to 4 because we don't need lane and lane norm\n",
    "#         print(\"here in argo\")\n",
    "#         print(\"before p_out \",data['p_out'])\n",
    "#         print(\"before v_out \",data['v_out'])\n",
    "#         for i in range(8):\n",
    "#             j = i % 2\n",
    "#             data[columns[i]][j] = (data[columns[i]][j] - varMinInput[i]) / (varMaxInput[i] - varMinInput[i])\n",
    "#             if i < 4 and self.training:\n",
    "#                 data[outColumns[i]][j] = (data[outColumns[i]][j] - varMinOutput[i]) / (varMaxOutput[i] - varMinOutput[i]) \n",
    "#         data['p_out'][0] = (data['p_out'][0] - -5.37872696e+00) / (4.72452881e+03 - -5.37872696e+00)\n",
    "#         print(\"after p_out \",data['p_out'])\n",
    "#         print(\"after v_out \",data['v_out'])\n",
    "        return data\n",
    "\n",
    "\n",
    "train_dataset  = ArgoverseDataset(data_path=new_path)\n",
    "val_dataset = ArgoverseDataset(data_path=val_path,training=False)\n",
    "#print((val_dataset[0]))\n",
    "#print(len(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a loader to enable batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 200\n",
    "def closest_node(node, nodes):\n",
    "    deltas = nodes - node\n",
    "    dist_2 = np.einsum('ij,ij->i', deltas, deltas)\n",
    "    return np.argmin(dist_2)\n",
    "\n",
    "def my_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    inpTotal = []\n",
    "    outTotal = []\n",
    "    inp = []\n",
    "    out = []\n",
    "    city = []\n",
    "#     print(\"pIn\",batch[0]['p_in'])\n",
    "    numbRows = 60\n",
    "    for scene in batch:\n",
    "#         print(\"scenePin\",scene['p_in'])\n",
    "        cityyy = numpy.zeros((60,1,8))\n",
    "        #print(cityyy)\n",
    "        if scene['city'] == 'PIT':\n",
    "            cityyy[:,:,:] = 1\n",
    "            #print(\"mummamia\",cityyy)\n",
    "        city.append(numpy.dstack([cityyy]))\n",
    "        laneInp = numpy.zeros((numbRows , 19,2))\n",
    "        lane_norm_inp = numpy.zeros((numbRows , 19,2))\n",
    "        laneOut = numpy.zeros((numbRows , 30,2))\n",
    "        lane_norm_out = numpy.zeros((numbRows , 30,2))\n",
    "        i = 0\n",
    "#         print(scene['p_in'])\n",
    "        while scene['car_mask'][i] == 1:\n",
    "             i+= 1\n",
    "        for x in range(i):\n",
    "            for y in range(19):\n",
    "                index = closest_node(scene['p_in'][x][y],scene['lane'][:,:2])\n",
    "                laneInp[x][y][0] = scene['lane'][index][0]\n",
    "                laneInp[x][y][1] = scene['lane'][index][1]\n",
    "                lane_norm_inp[x][y][0] = scene['lane_norm'][index][0]\n",
    "                lane_norm_inp[x][y][1] = scene['lane_norm'][index][1]\n",
    "            for y in range(30):\n",
    "                index = closest_node(scene['p_out'][x][y],scene['lane'][:,:2])\n",
    "                laneOut[x][y][0] = scene['lane'][index][0]\n",
    "                laneOut[x][y][1] = scene['lane'][index][1]\n",
    "                lane_norm_out[x][y][0] = scene['lane_norm'][index][0]\n",
    "                lane_norm_out[x][y][1] = scene['lane_norm'][index][1]\n",
    "\n",
    "        inpTotal.append(numpy.dstack([scene['p_in'],scene['v_in'],laneInp,lane_norm_inp]))\n",
    "        outTotal.append(numpy.dstack([scene['p_out'], scene['v_out'],laneOut,lane_norm_out]))\n",
    "#         inp.append(numpy.dstack([scene['p_in'],scene['v_in']]))\n",
    "        out.append(numpy.dstack([scene['p_out'], scene['v_out']]))\n",
    "\n",
    "#     inp = torch.FloatTensor(inp)\n",
    "    out = torch.FloatTensor(out)\n",
    "    inpTotal = torch.FloatTensor(inpTotal)\n",
    "    outTotal = torch.FloatTensor(outTotal)\n",
    "    city = torch.FloatTensor(city)\n",
    "    return [inpTotal,outTotal, out]\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz, shuffle = True, collate_fn=my_collate, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def val_collate(batch):\n",
    "    agentIds = []\n",
    "    trackIds = []\n",
    "    sceneIdxs = []\n",
    "    laneInfo = []\n",
    "    inp = []\n",
    "    city = []\n",
    "    numbRows = 60\n",
    "    for scene in batch:\n",
    "        \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "        agentIds.append(scene['agent_id'])\n",
    "        trackIds.append(scene['track_id'])\n",
    "        sceneIdxs.append(scene['scene_idx'])\n",
    "        \n",
    "        cityyy = numpy.zeros((60,1,4))\n",
    "        #print(cityyy)\n",
    "        if scene['city'] == 'PIT':\n",
    "            cityyy[:,:,:] = 1\n",
    "        city.append(numpy.dstack([cityyy]))\n",
    "        \n",
    "        lanes = numpy.zeros((numbRows * 30,2))\n",
    "        lane_norm = numpy.zeros((numbRows * 30,2))\n",
    "        lengthLane = min(numbRows * 30,len(scene['lane']))\n",
    "        lanes[:lengthLane,:2] = scene['lane'][:lengthLane,:2]\n",
    "        lane_norm[:lengthLane,:2] = scene['lane_norm'] [:lengthLane,:2]\n",
    "        laneInfo.append(numpy.dstack([lanes.reshape(60,30,2),lane_norm.reshape(60,30,2)]))\n",
    "        inp.append(numpy.dstack([scene['p_in'], scene['v_in']]))\n",
    "    inp = torch.FloatTensor(inp)\n",
    "    laneInfo = torch.FloatTensor(laneInfo)\n",
    "    city = torch.FloatTensor(city)\n",
    "    return [inp,sceneIdxs,agentIds,trackIds,laneInfo,city]\n",
    "\n",
    "val_loader = DataLoader(val_dataset,batch_size=batch_sz, shuffle = False, collate_fn=val_collate, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, self.hidden_dim, self.num_layers, batch_first=True,dropout=0.2)\n",
    "        self.fc = nn.Conv1d(self.hidden_dim, 240,1)\n",
    "#         self.bn1 = nn.BatchNorm1d(num_features=4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x,_ = self.lstm(x)\n",
    "#         print(\"shapeee\",x.shape)\n",
    "        x = x.transpose(1,2)\n",
    "#         print(x.shape)\n",
    "        x = self.fc(x)\n",
    "#         print(x.shape)\n",
    "        x = x.transpose(1,2)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def forwardTesting(self,x,num_steps=30):\n",
    "        res =[]\n",
    "        h = torch.zeros((self.num_layers,len(x),self.hidden_dim)).cuda()\n",
    "        c = torch.zeros((self.num_layers,len(x),self.hidden_dim)).cuda()\n",
    "        for steps in range(num_steps):\n",
    "            x,(h,c) = self.lstm(x,(h,c))\n",
    "            x = x[:,-1:]\n",
    "            x = x.transpose(1,2)\n",
    "            x = self.fc(x)\n",
    "            x = x.transpose(1,2)\n",
    "#             print(\"xxxx \" ,x)\n",
    "            res.append(x)\n",
    "        res = torch.cat(res,1)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(i_batch,sample_batch,loss_ema):        \n",
    "        inpTotal,outTotal, out = sample_batch\n",
    "#             laneInfo = laneInfo.cuda()\n",
    "        inpTotal = inpTotal.cuda()\n",
    "        outTotal = outTotal.cuda()\n",
    "#         city = city.cuda()\n",
    "#         inp = inp.cuda()\n",
    "        out = out.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        mixed = torch.cat([inpTotal,outTotal],2).transpose(1,2).reshape(-1,49,8 * 60)\n",
    "#         print(mixed.shape)\n",
    "        y_pred = model(mixed[:,:-1])[:,-30:]\n",
    "        y_pred = y_pred.reshape((-1,30,60,4)).transpose(1,2)\n",
    "\n",
    "#         mixedOut = torch.cat([inp,out],2).transpose(1,2).reshape((-1,60,30,4))\n",
    "#         print(y_pred.shape)\n",
    "        loss = (torch.mean((y_pred-out)**2))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if loss_ema < 0:\n",
    "            loss_ema = loss\n",
    "        loss_ema= loss_ema*0.90 +loss*0.1\n",
    "\n",
    "        if i_batch % 10 == 0:\n",
    "            print(\"batch #: \",i_batch * batch_sz ,\" avg loss per scene(past 100): \",loss_ema.item(),loss.item())\n",
    "            batch = []\n",
    "        return loss_ema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the batch of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "test\n",
      "batch #:  0  avg loss per scene(past 100):  358276.75 358276.75\n",
      "batch #:  2000  avg loss per scene(past 100):  338526.375 326595.3125\n",
      "batch #:  4000  avg loss per scene(past 100):  305296.3125 277710.21875\n",
      "batch #:  6000  avg loss per scene(past 100):  277852.65625 238458.8125\n",
      "batch #:  8000  avg loss per scene(past 100):  256201.3125 240327.484375\n",
      "batch #:  10000  avg loss per scene(past 100):  234020.65625 216770.453125\n",
      "batch #:  12000  avg loss per scene(past 100):  220312.125 235898.25\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "agent_id = 0\n",
    "learning_rate = 1e-2\n",
    "momentum = 0.5\n",
    "device = torch.device(\"cuda:0\")\n",
    "input_dim = 8 * 60    # input dimension\n",
    "hidden_dim = 3000  # hidden layer dimension should be greater when batch_sz small\n",
    "layer_dim = 3    # number of hidden layers\n",
    "output_dim = 4   # output dimension\n",
    "batch_sz = 200\n",
    "n_epochs = 30\n",
    "\n",
    "model = RNNModel(input_size=input_dim, output_size=output_dim, hidden_dim=hidden_dim, n_layers=layer_dim)\n",
    "#model.load_state_dict(torch.load('./sliding3epoch.pth'))\n",
    "model = model.to(device)\n",
    "# optimizer = optim.Adagrad(model.parameters(), lr=learning_rate,lr_decay=1e-6,weight_decay=1e-5)\n",
    "# optimizer = optim.Adadelta(model.parameters())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay=1e-5,amsgrad=True)\n",
    "#optimizer = optim.RMSprop(model.parameters(),lr=learning_rate,momentum=momentum)\n",
    "\n",
    "#find mean\n",
    "var = numpy.zeros((1,8))\n",
    "seen = 0\n",
    "print('start')\n",
    "\n",
    "    \n",
    "model.train()\n",
    "\n",
    "print(\"test\")\n",
    "# newOut = torch.zeros((batch_sz,100,30,4))\n",
    "batch = []\n",
    "batches = []\n",
    "loss_ema = -1\n",
    "ranThroughWholeDataSet = False\n",
    "try:\n",
    "    for i_epoch in range(n_epochs):\n",
    "        if ranThroughWholeDataSet == False:\n",
    "            for i_batch, sample_batch in enumerate(train_loader):\n",
    "#                 city,inpTotal,outTotal, out = sample_batch\n",
    "#                 print(inpTotal[0])\n",
    "                loss_ema = train(i_batch,sample_batch,loss_ema)\n",
    "                batches.append(sample_batch)\n",
    "            ranThroughWholeDataSet = True\n",
    "        else:\n",
    "            print(\"here\")\n",
    "            for i in range(len(batches)):\n",
    "                loss_ema = train(i,batches[i],loss_ema) \n",
    "        \n",
    "        torch.save(model.state_dict(), './sliding3epoch.pth')    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"savedModel\")\n",
    "    torch.save(model.state_dict(), './sliding3epoch.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './sliding3epoch.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "save_file = \"submissionsliding1.csv\"\n",
    "\n",
    "\n",
    "header = [\"ID\"]\n",
    "header += [\"v\"+str(x) for x in range(1, 61)]\n",
    "print(header)\n",
    "\n",
    "with open(save_file, 'w') as f:\n",
    "    f.write(\",\".join(header)+\"\\n\")\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "testmodel = RNNModel(input_size=input_dim, output_size=output_dim, hidden_dim=hidden_dim, n_layers=layer_dim)\n",
    "testmodel.load_state_dict(torch.load('further3000Temp.pth'))\n",
    "testmodel.to(device)\n",
    "testmodel.eval()\n",
    "\n",
    "full_out = []\n",
    "print(\"test\")\n",
    "varMaxOutput= [4773.,   4097.7,   193.19,  194.33]\n",
    "varMinOutput=[ -53.912,    0.,    -210.04,  -187.71 ]\n",
    "#         #Changed to 4 because we don't need lane and lane norm\n",
    "batch_sz = 1\n",
    "     \n",
    "for i_batch, sample_batch in enumerate(val_loader):\n",
    "    if i_batch % 100 == 0:\n",
    "        print(\"batch #: \", i_batch * batch_sz)\n",
    "\n",
    "    inp, scene_idx, agent_ids, track_ids,laneInfo,city = sample_batch\n",
    "    city = city.cuda()\n",
    "    laneInfo = laneInfo.cuda()\n",
    "    inp = inp.cuda()\n",
    "\n",
    "    mixed = torch.cat([city,laneInfo,inp],2).transpose(1,2).reshape(-1,50,4 * 60)\n",
    "    y_pred = testmodel.forwardTesting(mixed)\n",
    "    y_pred = y_pred.reshape((-1,30,60,4)).transpose(1,2)\n",
    "\n",
    "    i = 0\n",
    "    out = []\n",
    "\n",
    "    for ag_id in agent_ids:\n",
    "        nestedTrackIds = track_ids[i]\n",
    "        j = 0\n",
    "        for track_id in nestedTrackIds:\n",
    "\n",
    "            if ag_id == track_id[0]:\n",
    "                out.append(y_pred[i][j][:,:2].reshape(60))\n",
    "            j+=1\n",
    "        i+=1\n",
    "                           \n",
    " \n",
    "\n",
    "    with open(save_file, \"a\") as f:\n",
    "        for i in range(len(out)):\n",
    "#             realOut = out[i].eval(session=tf.compat.v1.Session())\n",
    "            array = out[i]\n",
    "#             print(out[i][0])\n",
    "            row = \"\"\n",
    "            for element in array:\n",
    "#                 print(element)\n",
    "                row += str(element.item()) + \",\"\n",
    "#             print(\"row \",row)\n",
    "#         for row, scene_num in zip(realOut.astype(int).astype(str), scene_idx):\n",
    "            f.write(str(scene_idx[i])+\",\" + row +\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
